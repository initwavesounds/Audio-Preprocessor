{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk86/mukv/IPhTsedXHcXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/officialcyber88/Audio-Preprocessor-GUI/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "F8TtMNs80KTM",
        "outputId": "36839a96-a586-4fac-c71f-30b4db555a14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# @title Mount Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G1UcUJdQOLzG"
      },
      "outputs": [],
      "source": [
        "# @title Install requirements\n",
        "!apt-get update -qq && apt-get install -y ffmpeg\n",
        "!pip install -q pydub numpy soundfile\n",
        "!pip install resampy\n",
        "!pip install gradio\n",
        "!pip install numpy librosa matplotlib soundfile pydub ipython pyloudnorm ffmpeg-normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Audio Preprocessor GUI\n",
        "\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import tempfile\n",
        "import warnings\n",
        "import pyloudnorm as pyln\n",
        "import subprocess\n",
        "from dataclasses import dataclass\n",
        "import gradio as gr\n",
        "import traceback\n",
        "from uuid import uuid4\n",
        "import zipfile\n",
        "import concurrent.futures\n",
        "\n",
        "# === Pre-flight Check: Install missing dependencies if needed ===\n",
        "try:\n",
        "    import resampy\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"resampy\"])\n",
        "    import resampy\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"])\n",
        "    import gdown\n",
        "\n",
        "# === Dependency Check ===\n",
        "if shutil.which(\"ffmpeg\") is None:\n",
        "    sys.stderr.write(\"Missing dependency: ffmpeg\\n\")\n",
        "    sys.exit(1)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# === Global Setup ===\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "OUTPUT_DIR = tempfile.mkdtemp(prefix=\"audio_preprocessor_\")\n",
        "print(f\"Audio output will be saved in: {OUTPUT_DIR}\")\n",
        "\n",
        "# === Constants ===\n",
        "DB_THRESH = -45\n",
        "EDGE_SILENCE_THRESHOLD = 3e-3\n",
        "VALID_FORMATS = ('.wav', '.mp3', '.flac', '.aiff', '.ogg', '.m4a')\n",
        "MIN_FRAMES_FOR_RMS = 50\n",
        "DEFAULT_HOP_LENGTH = 512\n",
        "\n",
        "# === Config Dataclass ===\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    sample_rate: int\n",
        "    bit_depth: str\n",
        "    channels: str\n",
        "    target_lufs: float\n",
        "    target_peak: float\n",
        "    use_cuda: bool\n",
        "    device: torch.device\n",
        "    visualize: bool\n",
        "    segmentation: bool\n",
        "    duration: float\n",
        "    panning: bool\n",
        "    mp3_bitrate: str\n",
        "    silence_trimming: bool  # Added silence trimming flag\n",
        "\n",
        "# === Helpers ===\n",
        "\n",
        "def pan_percent(l, r):\n",
        "    pl, pr = np.sum(l**2), np.sum(r**2)\n",
        "    t = pl + pr\n",
        "    if t < 1e-10:\n",
        "        return 50.0, 50.0, 0.0\n",
        "    return pl/t*100, pr/t*100, abs(pl/t*100 - 50)\n",
        "\n",
        "def calculate_adaptive_hop_length(L):\n",
        "    return min(DEFAULT_HOP_LENGTH, max(32, L // MIN_FRAMES_FOR_RMS))\n",
        "\n",
        "def measure_loudness(y, sr):\n",
        "    BLOCK = 0.4\n",
        "    if y.size == 0:\n",
        "        return {'lufs': None, 'peak': -np.inf}\n",
        "    y_m = np.mean(y, axis=0) if y.ndim > 1 else y\n",
        "    min_len = int(BLOCK * sr)\n",
        "    if len(y_m) < min_len:\n",
        "        y_m = np.pad(y_m, (0, min_len - len(y_m)))\n",
        "    meter = pyln.Meter(sr, block_size=BLOCK)\n",
        "    try:\n",
        "        lufs = meter.integrated_loudness(y_m)\n",
        "    except:\n",
        "        lufs = None\n",
        "    pk = np.max(np.abs(y))\n",
        "    pdb = 20 * np.log10(pk) if pk > 0 else -np.inf\n",
        "    return {'lufs': lufs, 'peak': pdb}\n",
        "\n",
        "def normalize_loudness(y, sr, log, tgt_lufs, tgt_peak):\n",
        "    if y.size == 0:\n",
        "        return y, {'method': 'empty'}\n",
        "    orig_pk = np.max(np.abs(y))\n",
        "    orig_db = 20 * np.log10(orig_pk) if orig_pk > 0 else -np.inf\n",
        "    y_m = np.mean(y, axis=0) if y.ndim > 1 else y\n",
        "    BLOCK = 0.4\n",
        "    min_len = max(int(BLOCK * sr), int(0.001 * sr))\n",
        "    if len(y_m) < min_len:\n",
        "        log(f\"[Normalize] Padding {min_len - len(y_m)} samples\")\n",
        "        y_m = np.pad(y_m, (0, min_len - len(y_m)))\n",
        "    meter = pyln.Meter(sr)\n",
        "    try:\n",
        "        orig_lufs = meter.integrated_loudness(y_m)\n",
        "        log(f\"[Normalize] Orig LUFS {orig_lufs:.2f}, Peak {orig_db:.2f}\")\n",
        "    except Exception as e:\n",
        "        log(f\"[Normalize] LUFS failed ({e}), peak-only\")\n",
        "        scale = (10 ** (tgt_peak / 20)) / orig_pk if orig_pk > 0 else 1\n",
        "        y_n = y * scale\n",
        "        fp = 20 * np.log10(np.max(np.abs(y_n))) if np.max(np.abs(y_n)) > 0 else -np.inf\n",
        "        return y_n, {\n",
        "            'original_lufs': None,\n",
        "            'original_peak': orig_db,\n",
        "            'normalized_lufs': None,\n",
        "            'normalized_peak': fp,\n",
        "            'method': 'peak_only'\n",
        "        }\n",
        "    gain = 10 ** ((tgt_lufs - orig_lufs) / 20)\n",
        "    y_l = y * gain\n",
        "    pk_after = np.max(np.abs(y_l))\n",
        "    if pk_after > 10 ** (tgt_peak / 20):\n",
        "        log(\"[Normalize] Limiting peak\")\n",
        "        y_n = y_l * (10 ** (tgt_peak / 20) / pk_after)\n",
        "    else:\n",
        "        y_n = y_l\n",
        "    fl = measure_loudness(y_n, sr)['lufs']\n",
        "    fp = 20 * np.log10(np.max(np.abs(y_n))) if np.max(np.abs(y_n)) > 0 else -np.inf\n",
        "    log(f\"[Normalize] Final LUFS {fl:.2f}, Peak {fp:.2f}\")\n",
        "    return y_n, {\n",
        "        'original_lufs': orig_lufs,\n",
        "        'original_peak': orig_db,\n",
        "        'normalized_lufs': fl,\n",
        "        'normalized_peak': fp,\n",
        "        'method': 'true_lufs'\n",
        "    }\n",
        "\n",
        "def normalize_panning(a, log):\n",
        "    if a.ndim != 2 or a.shape[0] != 2:\n",
        "        return a\n",
        "    lp, rp, _ = pan_percent(a[0], a[1])\n",
        "    log(f\"[Panning] Orig L{lp:.1f}% R{rp:.1f}%\")\n",
        "    r1, r2 = np.sqrt(np.mean(a[0]**2)), np.sqrt(np.mean(a[1]**2))\n",
        "    if r1 < 1e-7 or r2 < 1e-7:\n",
        "        return a\n",
        "    corr = np.vstack((a[0], a[1] * (r1 / r2)))\n",
        "    lp2, rp2, _ = pan_percent(corr[0], corr[1])\n",
        "    log(f\"[Panning] Corr L{lp2:.1f}% R{rp2:.1f}%\")\n",
        "    return corr\n",
        "\n",
        "def detect_clipping(y):\n",
        "    if y.size == 0:\n",
        "        return False, 0.0\n",
        "    c = np.sum(np.abs(y) >= 0.999) / y.size\n",
        "    return c > 0.001, c\n",
        "\n",
        "def attenuate_clipped_audio(y, log):\n",
        "    clip, ratio = detect_clipping(y)\n",
        "    if clip:\n",
        "        log(f\"[Clipping] {ratio:.1%} clipped; attenuating\")\n",
        "        pk = np.max(np.abs(y))\n",
        "        tgt = 10 ** (-1 / 20)\n",
        "        if pk > 0:\n",
        "            return y * (tgt / pk)\n",
        "    return y\n",
        "\n",
        "def auto_slice_audio(y, sr):\n",
        "    if y.size == 0:\n",
        "        return 0, 0\n",
        "    L = y.shape[-1]\n",
        "    if L < 128:\n",
        "        return 0, L\n",
        "    hop = calculate_adaptive_hop_length(L)\n",
        "    frame = min(2048, L)\n",
        "    rms = librosa.feature.rms(y=y, frame_length=frame, hop_length=hop)[0]\n",
        "    db = librosa.amplitude_to_db(rms, ref=np.max)\n",
        "    idx = np.where(db > DB_THRESH)[0]\n",
        "    if idx.size == 0:\n",
        "        return 0, L\n",
        "    return idx[0] * hop, min(L, (idx[-1] + 1) * hop)\n",
        "\n",
        "def process_silence(y, sr, log):\n",
        "    if y.size == 0:\n",
        "        return y, (0,0,0), (0,0)\n",
        "    L = y.shape[-1]\n",
        "    y_m = np.mean(y,axis=0) if y.ndim>1 else y\n",
        "    s0,e0 = auto_slice_audio(y_m, sr)\n",
        "    if e0<=s0:\n",
        "        log(\"[Silence] All silent\")\n",
        "        return np.array([]),(L/sr,0,L/sr),(0,0)\n",
        "    t = y[...,s0:e0]\n",
        "    tm = np.mean(t,axis=0) if t.ndim>1 else t\n",
        "    nz = np.where(np.abs(tm)>EDGE_SILENCE_THRESHOLD)[0]\n",
        "    if nz.size==0:\n",
        "        log(\"[Silence] All trimmed\")\n",
        "        return np.array([]),(L/sr,0,L/sr),(0,0)\n",
        "    fs,fe = nz[0],nz[-1]+1\n",
        "    final = t[...,fs:fe]\n",
        "    rem = L-final.shape[-1]\n",
        "    return final,((s0+fs)/sr,(L-(s0+fe))/sr,rem/sr),(s0+fs,s0+fe)\n",
        "\n",
        "def format_duration(s): return f\"{s:.3f}s\"\n",
        "\n",
        "def plot_zoomed_silence(y, sr, s0, e0, zoom=0.05):\n",
        "    zs = int(sr*zoom)\n",
        "    fig, axs = plt.subplots(2,1,figsize=(6,4))\n",
        "    pre = y[...,max(0,s0-zs):s0]\n",
        "    t0 = np.linspace(-zoom, 0, pre.shape[-1])\n",
        "    if pre.size>0: axs[0].plot(t0, pre.T if pre.ndim>1 else pre)\n",
        "    else: axs[0].text(0.5,0.5,\"No pre-silence\",ha='center')\n",
        "    axs[0].set_xlim(t0[0] if pre.size>0 else -zoom, 0)\n",
        "    axs[0].set_title(\"Zoomed Silence Pre-trim\")\n",
        "    post = y[...,e0:e0+zs]\n",
        "    t1 = np.linspace(0, zoom, post.shape[-1])\n",
        "    if post.size>0: axs[1].plot(t1, post.T if post.ndim>1 else post)\n",
        "    else: axs[1].text(0.5,0.5,\"No post-silence\",ha='center')\n",
        "    axs[1].set_xlim(0, t1[-1] if post.size>0 else zoom)\n",
        "    axs[1].set_title(\"Zoomed Silence Post-trim\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_waveform(y, sr, title, time_unit_str=\"s\", s0=None, e0=None, segments=None):\n",
        "    fig, ax = plt.subplots(figsize=(6,2.5))\n",
        "    t = np.arange(y.shape[-1])/sr\n",
        "    if y.ndim>1:\n",
        "        for c in range(y.shape[0]):\n",
        "            ax.plot(t, y[c], alpha=0.7, label=f'Ch{c+1}')\n",
        "        ax.legend(fontsize=\"small\")\n",
        "    else:\n",
        "        ax.plot(t, y)\n",
        "    if s0 is not None and e0 is not None:\n",
        "        ax.axvline(s0/sr, linestyle='--')\n",
        "        ax.axvline(e0/sr, linestyle='--')\n",
        "    if segments:\n",
        "        for st,en in segments:\n",
        "            ax.axvline(st/sr, linestyle='-', alpha=0.6)\n",
        "            ax.axvline(en/sr, linestyle='-', alpha=0.6)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(f\"Time ({time_unit_str})\")\n",
        "    ax.set_ylabel(\"Amplitude\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def get_all_audio_files(path):\n",
        "    files = []\n",
        "    if os.path.isfile(path) and path.lower().endswith(VALID_FORMATS):\n",
        "        files.append(path)\n",
        "    elif os.path.isdir(path):\n",
        "        for root, _, fnames in os.walk(path):\n",
        "            for f in fnames:\n",
        "                if f.lower().endswith(VALID_FORMATS):\n",
        "                    files.append(os.path.join(root, f))\n",
        "    elif os.path.isfile(path) and path.lower().endswith('.zip'):\n",
        "        tmp = tempfile.mkdtemp(prefix=\"zip_extract_\")\n",
        "        with zipfile.ZipFile(path, 'r') as zf:\n",
        "            zf.extractall(tmp)\n",
        "        for root, _, fnames in os.walk(tmp):\n",
        "            for f in fnames:\n",
        "                if f.lower().endswith(VALID_FORMATS):\n",
        "                    files.append(os.path.join(root, f))\n",
        "    return files\n",
        "\n",
        "def download_from_gdrive_folder(url, log):\n",
        "    m = re.search(r'/folders/([^/?]+)', url)\n",
        "    if not m:\n",
        "        log(\"❌ URL must be a shared FOLDER link\")\n",
        "        return None, None\n",
        "    fid = m.group(1)\n",
        "    parent = tempfile.mkdtemp(prefix=\"gdrive_dl_\")\n",
        "    outdir = os.path.join(parent, fid)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    log(f\"[GDrive] Downloading folder ID {fid} to {outdir}\")\n",
        "    gdown.download_folder(url=url, output=outdir, quiet=True, use_cookies=False)\n",
        "    subs = [d for d in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, d))]\n",
        "    if len(subs)==1:\n",
        "        det = subs[0]\n",
        "        log(f\"[GDrive] Detected folder name: {det}\")\n",
        "        return outdir, det\n",
        "    return outdir, fid\n",
        "\n",
        "def export_audio(y, sr, orig, fmt, cfg, exp_dir, idx=None):\n",
        "    base = os.path.splitext(os.path.basename(orig))[0]\n",
        "    name = f\"{base}_segment_{idx+1}.{fmt}\" if idx is not None else f\"{base}.{fmt}\"\n",
        "    out = os.path.join(exp_dir, name)\n",
        "    if y.size==0:\n",
        "        ch = 2 if cfg.channels=='stereo' else 1\n",
        "        y = np.zeros((ch,1)) if ch>1 else np.zeros(1)\n",
        "    dat = y.T if y.ndim>1 else y\n",
        "    subtype = f\"PCM_{cfg.bit_depth}\"\n",
        "    if fmt==\"mp3\":\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp:\n",
        "            sf.write(tmp.name, dat, sr, subtype=\"FLOAT\")\n",
        "            cmd = [\"ffmpeg\",\"-y\",\"-i\", tmp.name, \"-c:a\",\"libmp3lame\",\"-b:a\",cfg.mp3_bitrate,\"-ar\",str(sr), out]\n",
        "            subprocess.run(cmd, check=True, capture_output=True)\n",
        "    else:\n",
        "        if fmt==\"flac\" and cfg.bit_depth==\"32\":\n",
        "            subtype = \"PCM_24\"\n",
        "        sf.write(out, dat, sr, subtype=subtype)\n",
        "    return out\n",
        "\n",
        "def process_file(fp, cfg, params):\n",
        "    res = {'messages':[], 'export_paths':[], 'plot_data':None}\n",
        "    def log(m): res['messages'].append(m)\n",
        "    try:\n",
        "        log(f\"--- Processing {os.path.basename(fp)} ---\")\n",
        "        log(f\"Device: {'GPU' if cfg.use_cuda else 'CPU'}\")\n",
        "        y, orig_sr = librosa.load(fp, sr=None, mono=False)\n",
        "        if y.ndim==1: y = y[np.newaxis,:]\n",
        "        info = sf.info(fp)\n",
        "        log(f\"Input: {info.samplerate}Hz, {info.channels}ch, {format_duration(info.duration)}\")\n",
        "        if cfg.panning and y.shape[0]==2:\n",
        "            y = normalize_panning(y, log)\n",
        "        if orig_sr!=cfg.sample_rate:\n",
        "            log(f\"Resampling {orig_sr}→{cfg.sample_rate}\")\n",
        "            y = resampy.resample(y, orig_sr, cfg.sample_rate)\n",
        "        if y.ndim==1: y = y[np.newaxis,:]\n",
        "        if cfg.channels==\"mono\" and y.shape[0]>1:\n",
        "            log(\"Converting to mono\")\n",
        "            y = np.mean(y, axis=0, keepdims=True)\n",
        "        elif cfg.channels==\"stereo\" and y.shape[0]==1:\n",
        "            y = np.vstack([y,y])\n",
        "        y = attenuate_clipped_audio(y, log)\n",
        "        y_pre = y.copy()\n",
        "\n",
        "        # Apply silence trimming if enabled\n",
        "        if cfg.silence_trimming:  # New conditional check\n",
        "            y_proc, (_pre,_post,total), (s0,e0) = process_silence(y, cfg.sample_rate, log)\n",
        "            log(f\"Silence removed {format_duration(total)} (start {format_duration(_pre)}, end {format_duration(_post)})\")\n",
        "        else:\n",
        "            y_proc = y\n",
        "            s0, e0 = 0, y.shape[-1]\n",
        "            log(\"Silence trimming skipped\")\n",
        "\n",
        "        if y_proc.size==0:\n",
        "            log(\"Empty after silence; skipping\")\n",
        "            return res\n",
        "\n",
        "        segments = [(0, y_proc.shape[-1])]\n",
        "        if cfg.segmentation:\n",
        "            dur_sec = y_proc.shape[-1]/cfg.sample_rate\n",
        "            if dur_sec < cfg.duration:\n",
        "                log(\"⚠ shorter than segment duration; skipping export\")\n",
        "                return res\n",
        "            ss = int(cfg.sample_rate * cfg.duration)\n",
        "            nseg = int(np.ceil(dur_sec / cfg.duration))\n",
        "            segments = [(i*ss, min((i+1)*ss, y_proc.shape[-1])) for i in range(nseg)]\n",
        "            log(f\"Segmenting into {format_duration(cfg.duration)}, created {len(segments)} segments\")\n",
        "\n",
        "        res['plot_data'] = {'y_pre':y_pre, 'y_proc':y_proc, 'sr':cfg.sample_rate, 's0':s0, 'e0':e0, 'segments':segments}\n",
        "\n",
        "        for i,(st,en) in enumerate(segments):\n",
        "            seg = y_proc[..., st:en]\n",
        "            if params['normalize']!=\"No Normalization\":\n",
        "                log(f\"Normalizing {'segment '+str(i+1) if cfg.segmentation else 'file'}\")\n",
        "                seg, _ = normalize_loudness(seg, cfg.sample_rate, log, cfg.target_lufs, cfg.target_peak)\n",
        "            out_path = export_audio(seg, cfg.sample_rate, fp, params['out_fmt'], cfg, OUTPUT_DIR, idx=(i if cfg.segmentation else None))\n",
        "            res['export_paths'].append(out_path)\n",
        "            log(f\"✅ Exported {os.path.basename(out_path)}\")\n",
        "        return res\n",
        "\n",
        "    except Exception:\n",
        "        tb = traceback.format_exc()\n",
        "        res['messages'].append(f\"❌ ERROR:\\n{tb}\")\n",
        "        return res\n",
        "\n",
        "def gradio_process(input_mode, uploads, path_in, gdrive_url,\n",
        "                  out_fmt, sr, bd, ch, mp3_br,\n",
        "                  norm, pan, seg, dur, tu, viz,\n",
        "                  zip_enable, custom_zip_name, silence_trimming):  # Added silence_trimming parameter\n",
        "    logs = [\"=== Input Method ===\"]\n",
        "    raw_inputs = []\n",
        "    base_name_for_zip = None\n",
        "\n",
        "    if input_mode==\"Path\" and path_in.strip():\n",
        "        logs.append(f\"Mode: Path ➞ {path_in}\")\n",
        "        raw_inputs = get_all_audio_files(path_in.strip())\n",
        "        base_name_for_zip = os.path.basename(path_in.rstrip(\"/\"))\n",
        "    elif input_mode==\"Google Drive URL\" and gdrive_url.strip():\n",
        "        downloaded, detected = download_from_gdrive_folder(gdrive_url.strip(), logs.append)\n",
        "        if not downloaded:\n",
        "            return \"\\n\".join(logs+[\"❌ Aborting: invalid Google Drive URL.\"]), [], [], gr.update(choices=[], value=None), None\n",
        "        logs.append(f\"Mode: Google Drive URL ➞ {gdrive_url}\")\n",
        "        base_name_for_zip = detected\n",
        "        for root,_,_ in os.walk(downloaded):\n",
        "            raw_inputs.extend(get_all_audio_files(root))\n",
        "    else:\n",
        "        logs.append(f\"Mode: Upload ➞ {len(uploads) if uploads else 0} file(s)\")\n",
        "        raw_inputs = [f.name for f in uploads] if uploads else []\n",
        "        base_name_for_zip = None\n",
        "\n",
        "    logs.append(\"=== Original File Details ===\")\n",
        "    for fp in raw_inputs:\n",
        "        try:\n",
        "            info = sf.info(fp)\n",
        "            logs.append(f\"{os.path.basename(fp)}: {info.samplerate}Hz, {info.channels}ch, {format_duration(info.duration)}\")\n",
        "        except:\n",
        "            logs.append(f\"{os.path.basename(fp)}: <could not read metadata>\")\n",
        "    logs.append(\"\")\n",
        "\n",
        "    logs.append(\"=== Output Settings ===\")\n",
        "    logs.append(f\"Format: {out_fmt}\")\n",
        "    logs.append(f\"Sample Rate: {sr}\")\n",
        "    if out_fmt.lower()==\"mp3\":\n",
        "        logs.append(f\"MP3 Bitrate: {mp3_br}\")\n",
        "    else:\n",
        "        logs.append(f\"Bit Depth: {bd}\")\n",
        "    logs.append(f\"Channels: {ch}\")\n",
        "    logs.append(\"\")\n",
        "\n",
        "    export_bd = bd\n",
        "    if out_fmt==\"flac\" and bd==\"32\":\n",
        "        logs.append(\"⚠ FLAC does not support 32-bit; falling back to 24-bit\")\n",
        "        export_bd = \"24\"\n",
        "\n",
        "    logs.append(\"=== Processing Options ===\")\n",
        "    logs.append(f\"Normalization Profile: {norm}\")\n",
        "    logs.append(f\"Panning Correction: {pan}\")\n",
        "    logs.append(f\"Silence Trimming: {silence_trimming}\")  # New log entry\n",
        "    logs.append(f\"Segmentation: {'Yes' if seg else 'No'}\" + (f\", Duration {dur}{tu}\" if seg else \"\"))\n",
        "    logs.append(f\"Show Visualizations: {'Yes' if viz else 'No'}\")\n",
        "    logs.append(f\"Custom ZIP Name: {custom_zip_name or '(none)'}\")\n",
        "    logs.append(\"\")\n",
        "\n",
        "    if not raw_inputs:\n",
        "        return \"\\n\".join(logs+[\"❌ No audio files provided.\"]), [], [], gr.update(choices=[], value=None), None\n",
        "\n",
        "    params = {'out_fmt': out_fmt, 'normalize': norm}\n",
        "    tgt_lufs, tgt_peak = {\"Spotify\":(-14.0,-1.0), \"Apple Music\":(-16.0,-1.0)}.get(norm, (None, None))\n",
        "    raw = float(dur)\n",
        "    if tu==\"Milliseconds\": dsec = raw/1000\n",
        "    elif tu==\"Minutes\":\n",
        "        dsec = raw*60\n",
        "    elif tu==\"Hours\":\n",
        "        dsec = raw*3600\n",
        "    else:\n",
        "        dsec = raw\n",
        "\n",
        "    cfg = Config(\n",
        "        sample_rate=int(sr.replace(\"Hz\",\"\")),\n",
        "        bit_depth=export_bd, channels=ch,\n",
        "        target_lufs=tgt_lufs, target_peak=tgt_peak,\n",
        "        use_cuda=use_cuda, device=device,\n",
        "        visualize=viz, segmentation=seg,\n",
        "        duration=dsec, panning=(pan==\"Yes\"),\n",
        "        mp3_bitrate=mp3_br,\n",
        "        silence_trimming=(silence_trimming==\"Yes\")  # New config parameter\n",
        "    )\n",
        "\n",
        "    gallery_images = []\n",
        "    export_paths = []\n",
        "\n",
        "    workers = os.cpu_count() or 1\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        futures = {executor.submit(process_file, fp, cfg, params): fp for fp in raw_inputs}\n",
        "        for fut in concurrent.futures.as_completed(futures):\n",
        "            r = fut.result()\n",
        "            logs.extend(r['messages'])\n",
        "            logs.append(\"\")\n",
        "            export_paths.extend(r['export_paths'])\n",
        "            if viz and r.get('plot_data'):\n",
        "                pd = r['plot_data']\n",
        "                figs = [\n",
        "                    (plot_waveform(pd['y_pre'], pd['sr'], \"Original w/ Trim\", tu[0], pd['s0'], pd['e0'], pd['segments']), 'pre'),\n",
        "                    (plot_waveform(pd['y_proc'], pd['sr'], \"Processed Output\", tu[0]), 'post'),\n",
        "                    (plot_zoomed_silence(pd['y_pre'], pd['sr'], pd['s0'], pd['e0']), 'zoom')\n",
        "                ]\n",
        "                for fig, tag in figs:\n",
        "                    fn = os.path.join(OUTPUT_DIR, f\"{uuid4().hex}_{tag}.png\")\n",
        "                    fig.savefig(fn)\n",
        "                    gallery_images.append(fn)\n",
        "\n",
        "    play_paths = []\n",
        "    for p in export_paths:\n",
        "        if p.lower().endswith('.flac'):\n",
        "            wav_play = p[:-5] + '_playback.wav'\n",
        "            y, sr_load = sf.read(p)\n",
        "            sf.write(wav_play, y, sr_load)\n",
        "            play_paths.append(wav_play)\n",
        "        else:\n",
        "            play_paths.append(p)\n",
        "\n",
        "    if zip_enable and export_paths:\n",
        "        if custom_zip_name.strip():\n",
        "            zip_base = custom_zip_name.strip()\n",
        "        elif base_name_for_zip:\n",
        "            zip_base = base_name_for_zip\n",
        "        else:\n",
        "            zip_base = os.path.splitext(os.path.basename(raw_inputs[0]))[0]\n",
        "        zip_filename = f\"{zip_base}.zip\"\n",
        "        zip_path = os.path.join(OUTPUT_DIR, zip_filename)\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "            for ex in export_paths:\n",
        "                arc = os.path.relpath(ex, OUTPUT_DIR)\n",
        "                zf.write(ex, arcname=arc)\n",
        "        download_paths = [zip_path]\n",
        "    else:\n",
        "        download_paths = export_paths\n",
        "\n",
        "    logs.append(\"=== Exported Files ===\")\n",
        "    logs.append(f\"Count: {len(download_paths)}\")\n",
        "    logs.append(\"--- Used Settings ---\")\n",
        "    logs.append(f\"Format: {out_fmt}\")\n",
        "    logs.append(f\"Sample Rate: {sr}\")\n",
        "    if out_fmt.lower()==\"mp3\":\n",
        "        logs.append(f\"MP3 Bitrate: {mp3_br}\")\n",
        "    else:\n",
        "        logs.append(f\"Bit Depth: {export_bd}\")\n",
        "    logs.append(f\"Channels: {ch}\")\n",
        "    logs.append(f\"Normalization Profile: {norm}\")\n",
        "    logs.append(f\"Panning Correction: {pan}\")\n",
        "    logs.append(f\"Silence Trimming: {silence_trimming}\")  # New log entry\n",
        "    logs.append(f\"Segmentation: {'Yes' if seg else 'No'}\" + (f\", Duration {dur}{tu}\" if seg else \"\"))\n",
        "    logs.append(f\"Visualizations: {'Yes' if viz else 'No'}\")\n",
        "    logs.append(\"\")\n",
        "    for fn in download_paths:\n",
        "        logs.append(os.path.basename(fn))\n",
        "\n",
        "    default_play = play_paths[0] if play_paths else None\n",
        "    dropdown_update = gr.update(choices=play_paths, value=default_play)\n",
        "\n",
        "    return (\n",
        "        \"\\n\".join(logs),\n",
        "        gallery_images,\n",
        "        download_paths,\n",
        "        dropdown_update,\n",
        "        default_play\n",
        "    )\n",
        "\n",
        "# === Gradio UI ===\n",
        "with gr.Blocks(title=\"Audio Preprocessor GUI\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Audio Preprocessor\")\n",
        "    gr.Markdown(f\"Outputs saved in `{OUTPUT_DIR}`\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            input_mode     = gr.Radio([\"Upload\",\"Path\",\"Google Drive URL\"], value=\"Upload\", label=\"Input Method\")\n",
        "            file_uploader  = gr.File(file_count=\"multiple\", file_types=[\"audio\"], label=\"Upload Audio Files\")\n",
        "            path_text      = gr.Textbox(placeholder=\"/path/to/dir\", label=\"Or Enter Path\", visible=False)\n",
        "            gdrive_text    = gr.Textbox(placeholder=\"URL Link\", label=\"Or Enter Google Drive FOLDER URL\", visible=False)\n",
        "            input_mode.change(\n",
        "                lambda m: (\n",
        "                    gr.update(visible=(m==\"Upload\")),\n",
        "                    gr.update(visible=(m==\"Path\")),\n",
        "                    gr.update(visible=(m==\"Google Drive URL\"))\n",
        "                ),\n",
        "                inputs=[input_mode],\n",
        "                outputs=[file_uploader, path_text, gdrive_text]\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### Output Settings\")\n",
        "            with gr.Row():\n",
        "                out_fmt     = gr.Dropdown([\"wav\",\"mp3\",\"flac\",\"aiff\"],    value=\"wav\",      label=\"Format\")\n",
        "                sample_rate = gr.Dropdown([\"16000Hz\",\"44100Hz\",\"48000Hz\"], value=\"48000Hz\", label=\"Sample Rate\")\n",
        "                bit_depth   = gr.Dropdown([\"16\",\"24\",\"32\"],               value=\"24\",       label=\"Bit Depth\")\n",
        "            with gr.Row():\n",
        "                channels    = gr.Radio([\"mono\",\"stereo\"],                value=\"mono\",     label=\"Channels\")\n",
        "                mp3_bitrate = gr.Dropdown([\"128k\",\"192k\",\"256k\",\"320k\"],  value=\"192k\",     label=\"MP3 Bitrate\")\n",
        "\n",
        "    with gr.Accordion(\"Processing Options\", open=True):\n",
        "        with gr.Row():\n",
        "            norm_profile   = gr.Dropdown([\"No Normalization\",\"Spotify\",\"Apple Music\"], value=\"Spotify\", label=\"Normalization Profile\")\n",
        "            panning_option = gr.Dropdown([\"Yes\",\"No\"],                             value=\"Yes\",     label=\"Enable Panning Correction\")\n",
        "            silence_trimming = gr.Dropdown([\"Yes\",\"No\"],                           value=\"Yes\",     label=\"Enable Silence Trimming\")  # New dropdown\n",
        "            visualize      = gr.Checkbox(value=True, label=\"Show Visualizations\")\n",
        "        with gr.Row():\n",
        "            segmentation = gr.Checkbox(value=False, label=\"Enable Segmentation\")\n",
        "            duration     = gr.Slider(minimum=1, maximum=60, step=1, value=30, label=\"Segment Duration\")\n",
        "            time_unit    = gr.Dropdown([\"Milliseconds\",\"Seconds\",\"Minutes\",\"Hours\"], value=\"Seconds\", label=\"Time Unit\")\n",
        "        with gr.Row():\n",
        "            zip_enable      = gr.Checkbox(value=True, label=\"Save outputs as ZIP\")\n",
        "            custom_zip_name = gr.Textbox(placeholder=\"Enter ZIP name (without .zip)\", label=\"Custom ZIP Name (optional)\")\n",
        "\n",
        "    process_btn = gr.Button(\"Process Audio\", variant=\"primary\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Logs\"):\n",
        "            logs_out = gr.Textbox(lines=15, label=\"Processing Logs\", interactive=False)\n",
        "        with gr.TabItem(\"Visualizations\"):\n",
        "            gr.Markdown(\"All waveform plots (3 per file)\")\n",
        "            gallery  = gr.Gallery(label=\"Plots\", columns=3, height=\"auto\")\n",
        "        with gr.TabItem(\"Output Files\"):\n",
        "            audio_out = gr.File(file_count=\"multiple\", label=\"Processed Audio Files\", interactive=False)\n",
        "        with gr.TabItem(\"Audio Player\"):\n",
        "            file_selector = gr.Dropdown(choices=[], label=\"Select File to Play\")\n",
        "            audio_player  = gr.Audio(label=\"Play Processed Audio\", interactive=True)\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=gradio_process,\n",
        "        inputs=[\n",
        "            input_mode, file_uploader, path_text, gdrive_text,\n",
        "            out_fmt, sample_rate, bit_depth, channels, mp3_bitrate,\n",
        "            norm_profile, panning_option, segmentation, duration,\n",
        "            time_unit, visualize, zip_enable, custom_zip_name,\n",
        "            silence_trimming  # Added new input\n",
        "        ],\n",
        "        outputs=[logs_out, gallery, audio_out, file_selector, audio_player]\n",
        "    )\n",
        "\n",
        "    file_selector.change(fn=lambda f: f, inputs=file_selector, outputs=audio_player)\n",
        "\n",
        "    # Enable Gradio queueing to avoid HTTP timeouts\n",
        "    demo.queue()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "i0h71GKL1yri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Audio Preprocessor Manual\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import IPython.display as ipd\n",
        "from pydub import AudioSegment\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import tempfile\n",
        "import warnings\n",
        "import pyloudnorm as pyln\n",
        "import subprocess\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# === 0. Dependency Check ===\n",
        "def check_dependencies():\n",
        "    \"\"\"Ensure all external binaries and Python packages are available.\"\"\"\n",
        "    missing = []\n",
        "    if shutil.which(\"ffmpeg\") is None:\n",
        "        missing.append(\"ffmpeg (binary on PATH)\")\n",
        "    if missing:\n",
        "        sys.stderr.write(\"Missing dependencies:\\n\")\n",
        "        for m in missing:\n",
        "            sys.stderr.write(f\"  • {m}\\n\")\n",
        "        sys.exit(1)\n",
        "\n",
        "check_dependencies()\n",
        "\n",
        "# Suppress librosa warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# === Check GPU Availability ===\n",
        "use_cuda_global = torch.cuda.is_available()\n",
        "device_global = torch.device('cuda' if use_cuda_global else 'cpu')\n",
        "print(f\"[Info] Using {'CUDA GPU' if use_cuda_global else 'CPU'} for processing\")\n",
        "print(f\"[Info] Device: {device_global}\")\n",
        "cpu_count = os.cpu_count() or 1\n",
        "print(f\"[Info] Detected {cpu_count} CPU cores → Using \"\n",
        "      f\"{max(1, min(cpu_count // 2, 4)) if use_cuda_global else max(1, min(cpu_count, 8))} workers\")\n",
        "print()  # Blank line\n",
        "\n",
        "# === Audio Processing Configuration ===\n",
        "# @markdown ---\n",
        "# @markdown ## Input Settings\n",
        "input_path = \"\"  # @param {type:\"string\"}\n",
        "output_path = \"/content/audio_preprocessor\"\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Audio Format Settings\n",
        "out_format = \"mp3\"  # @param [\"wav\", \"mp3\", \"flac\", \"aiff\"]\n",
        "sample_rate = \"48000\"  # @param [\"16000\", \"41000\", \"48000\", \"96000\"]\n",
        "bit_depth = \"24\"  # @param [\"16\", \"24\", \"32\"]\n",
        "channels = \"mono\"  # @param [\"mono\", \"stereo\"]\n",
        "mp3_bitrate = \"320k\"  # @param [\"128k\", \"192k\", \"320k\"]\n",
        "normalization_profile = \"Spotify\"  # @param [\"Spotify\", \"default\"]\n",
        "# Define platform-specific loudness targets\n",
        "profile_settings = {\n",
        "    \"Spotify\": {\"lufs\": -14.0, \"peak\": -1.0},\n",
        "    \"default\": {\"lufs\": None, \"peak\": None}  # No change\n",
        "}\n",
        "\n",
        "# Set target values based on profile\n",
        "if normalization_profile == \"default\":\n",
        "    print(\"[Info] Default selected: using original audio LUFS and Peak.\")\n",
        "    TARGET_LUFS = None\n",
        "    TARGET_PEAK_DB = None\n",
        "    normalize_audio = False  # Disable normalization for default\n",
        "else:\n",
        "    TARGET_LUFS = profile_settings[normalization_profile][\"lufs\"]\n",
        "    TARGET_PEAK_DB = profile_settings[normalization_profile][\"peak\"]\n",
        "    normalize_audio = True   # Enable normalization for Spotify\n",
        "    print(f\"[Normalize] Target: {TARGET_LUFS} LUFS, {TARGET_PEAK_DB} dBTP for {normalization_profile}\")\n",
        "\n",
        "# User setting: panning correction enabled or not (Yes/No)\n",
        "panning_correction = \"Yes\"  # @param [\"Yes\", \"No\"]\n",
        "# Convert to boolean\n",
        "panning_correction = (panning_correction == \"Yes\")\n",
        "if panning_correction:\n",
        "    print(\"[Info] Panning correction is enabled.\")\n",
        "else:\n",
        "    print(\"[Info] Panning correction is disabled.\")\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Silence Trimming\n",
        "apply_silence_trimming = \"Yes\"  # @param [\"Yes\", \"No\"]\n",
        "\n",
        "visualize = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Segmentation Options\n",
        "segmentation = False  # @param {type:\"boolean\"}\n",
        "duration = 30  # @param {type:\"slider\", min:1, max:60, step:1}\n",
        "time_unit = \"Seconds\"  # @param [\"Milliseconds\", \"Seconds\", \"Minutes\", \"Hours\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ## Output Options\n",
        "zip_output = \"No\"  # @param [\"Yes\", \"No\"]\n",
        "\n",
        "# === Auto Worker Configuration ===\n",
        "num_workers = max(1, min(cpu_count // 2, 4)) if use_cuda_global else max(1, min(cpu_count, 8))\n",
        "\n",
        "# === Directory Handling ===\n",
        "format_subfolder = {'wav': 'wav', 'mp3': 'mp3', 'flac': 'flac', 'aiff': 'aiff'}[out_format]\n",
        "export = os.path.join(output_path, format_subfolder)\n",
        "os.makedirs(export, exist_ok=True)\n",
        "\n",
        "# === Constants ===\n",
        "TOP_DB = 40\n",
        "DB_THRESH = -45\n",
        "EDGE_SILENCE_THRESHOLD = 3e-3\n",
        "VALID_FORMATS = ('.wav', '.mp3', '.flac', '.aiff')\n",
        "MIN_FRAMES_FOR_RMS = 50\n",
        "DEFAULT_HOP_LENGTH = 512\n",
        "\n",
        "# === Loudness Normalization Helpers ===\n",
        "MIN_BLOCK_SIZE = 0.050  # 50 ms\n",
        "MIN_BLOCK_FLOOR = 0.001  # 1 ms\n",
        "\n",
        "# === 1. Centralized Config ===\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    sample_rate: int\n",
        "    bit_depth: str\n",
        "    channels: str\n",
        "    normalization_profile: str\n",
        "    TARGET_LUFS: float\n",
        "    TARGET_PEAK_DB: float\n",
        "    use_cuda: bool\n",
        "    device: torch.device\n",
        "    visualize: bool\n",
        "    segmentation: bool\n",
        "    duration_seconds: float\n",
        "    panning_correction: bool\n",
        "    mp3_bitrate: str\n",
        "    apply_silence_trimming: bool\n",
        "\n",
        "cfg = Config(\n",
        "    sample_rate=int(sample_rate),\n",
        "    bit_depth=bit_depth,\n",
        "    channels=channels,\n",
        "    normalization_profile=normalization_profile,\n",
        "    TARGET_LUFS=TARGET_LUFS,\n",
        "    TARGET_PEAK_DB=TARGET_PEAK_DB,\n",
        "    use_cuda=use_cuda_global,\n",
        "    device=device_global,\n",
        "    visualize=visualize,\n",
        "    segmentation=segmentation,\n",
        "    duration_seconds=(\n",
        "        duration if time_unit == \"Seconds\"\n",
        "        else duration / 1000.0 if time_unit == \"Milliseconds\"\n",
        "        else duration * 60 if time_unit == \"Minutes\"\n",
        "        else duration * 3600\n",
        "    ),\n",
        "    panning_correction=panning_correction,\n",
        "    mp3_bitrate=mp3_bitrate,\n",
        "    apply_silence_trimming=(apply_silence_trimming == \"Yes\")\n",
        ")\n",
        "\n",
        "def measure_loudness(y: np.ndarray, sr: int) -> dict:\n",
        "    \"\"\"\n",
        "    Measure integrated LUFS and peak dB of an audio array.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.ndarray\n",
        "        Audio samples (mono or multichannel).\n",
        "    sr : int\n",
        "        Sample rate in Hz.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        { 'lufs': float | None, 'peak': float }\n",
        "    \"\"\"\n",
        "    BLOCK_SECONDS = 0.400\n",
        "    y_mono = np.mean(y, axis=0) if y.ndim > 1 else y\n",
        "    min_len = int(BLOCK_SECONDS * sr)\n",
        "    if len(y_mono) < min_len:\n",
        "        y_mono_padded = np.pad(y_mono, (0, min_len - len(y_mono)), mode=\"constant\")\n",
        "    else:\n",
        "        y_mono_padded = y_mono\n",
        "    meter = pyln.Meter(sr, block_size=BLOCK_SECONDS)\n",
        "    try:\n",
        "        lufs = meter.integrated_loudness(y_mono_padded)\n",
        "    except:\n",
        "        lufs = None\n",
        "    peak_amp = np.max(np.abs(y))\n",
        "    peak_db = 20 * np.log10(peak_amp) if peak_amp > 0 else -np.inf\n",
        "    return {'lufs': lufs, 'peak': peak_db}\n",
        "\n",
        "def normalize_loudness_true(y: np.ndarray, sr: int, log_message_func) -> tuple:\n",
        "    \"\"\"\n",
        "    True-LUFS normalization with peak limiting fallback.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.ndarray\n",
        "    sr : int\n",
        "    log_message_func : callable\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        y_norm : np.ndarray\n",
        "        report : dict\n",
        "    \"\"\"\n",
        "    BLOCK_SECONDS = 0.400\n",
        "    orig_peak_amp = np.max(np.abs(y))\n",
        "    orig_peak_db = 20 * np.log10(orig_peak_amp) if orig_peak_amp > 0 else -np.inf\n",
        "    log_message_func(f\"[Normalize] Original Peak: {orig_peak_db:.2f} dBFS\")\n",
        "    y_mono = np.mean(y, axis=0) if y.ndim > 1 else y\n",
        "    min_len = int(BLOCK_SECONDS * sr)\n",
        "    if len(y_mono) < min_len:\n",
        "        y_mono_padded = np.pad(y_mono, (0, min_len - len(y_mono)), mode=\"constant\")\n",
        "    else:\n",
        "        y_mono_padded = y_mono\n",
        "    meter = pyln.Meter(sr, block_size=BLOCK_SECONDS)\n",
        "    try:\n",
        "        orig_lufs = meter.integrated_loudness(y_mono_padded)\n",
        "        log_message_func(f\"[Normalize] Original LUFS: {orig_lufs:.2f} LUFS\")\n",
        "    except Exception as e:\n",
        "        log_message_func(f\"[Normalize] LUFS measurement failed: {e}\")\n",
        "        return normalize_by_peak_only(y, sr, log_message_func)\n",
        "    # Use global TARGET_LUFS instead of cfg.TARGET_LUFS\n",
        "    gain_lin = 10 ** ((TARGET_LUFS - orig_lufs) / 20)\n",
        "    y_lufs = y * gain_lin\n",
        "    log_message_func(f\"[Normalize] Applied gain: {TARGET_LUFS - orig_lufs:.2f} dB to reach target LUFS\")\n",
        "    peak_amp_after = np.max(np.abs(y_lufs))\n",
        "    peak_db_after = 20 * np.log10(peak_amp_after) if peak_amp_after > 0 else -np.inf\n",
        "    log_message_func(f\"[Normalize] Peak after LUFS: {peak_db_after:.2f} dBFS\")\n",
        "    # Use global TARGET_PEAK_DB instead of cfg.TARGET_PEAK_DB\n",
        "    if peak_db_after > TARGET_PEAK_DB:\n",
        "        scale = (10 ** (TARGET_PEAK_DB / 20)) / peak_amp_after\n",
        "        y_norm = y_lufs * scale\n",
        "        log_message_func(f\"[Normalize] Applied peak limit to {TARGET_PEAK_DB:.1f} dBFS\")\n",
        "    else:\n",
        "        y_norm = y_lufs\n",
        "        log_message_func(\"[Normalize] No peak limiting needed\")\n",
        "    y_norm_mono = np.mean(y_norm, axis=0) if y_norm.ndim > 1 else y_norm\n",
        "    if len(y_norm_mono) < min_len:\n",
        "        y_norm_mono_padded = np.pad(y_norm_mono, (0, min_len - len(y_norm_mono)), mode=\"constant\")\n",
        "    else:\n",
        "        y_norm_mono_padded = y_norm\n",
        "    try:\n",
        "        final_lufs = meter.integrated_loudness(y_norm_mono_padded)\n",
        "        log_message_func(f\"[Normalize] Final LUFS: {final_lufs:.2f} LUFS\")\n",
        "    except:\n",
        "        final_lufs = None\n",
        "    final_peak_amp = np.max(np.abs(y_norm))\n",
        "    final_peak_db = 20 * np.log10(final_peak_amp) if final_peak_amp > 0 else -np.inf\n",
        "    log_message_func(f\"[Normalize] Final Peak: {final_peak_db:.2f} dBFS\")\n",
        "    return y_norm, {\n",
        "        'original_lufs':   orig_lufs,\n",
        "        'original_peak':   orig_peak_db,\n",
        "        'normalized_lufs': final_lufs,\n",
        "        'normalized_peak': final_peak_db,\n",
        "        'method':          'true_lufs',\n",
        "        'duration':        len(y_norm) / sr\n",
        "    }\n",
        "\n",
        "def normalize_by_peak_only(y: np.ndarray, sr: int, log_message_func) -> tuple:\n",
        "    \"\"\"\n",
        "    Peak-only normalization fallback.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.ndarray\n",
        "    sr : int\n",
        "    log_message_func : callable\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        y_norm : np.ndarray\n",
        "        report : dict\n",
        "    \"\"\"\n",
        "    log_message_func(\"[Normalize] Applying peak-only normalization\")\n",
        "    orig_peak = np.max(np.abs(y))\n",
        "    orig_peak_db = 20 * np.log10(orig_peak) if orig_peak > 0 else -np.inf\n",
        "    log_message_func(f\"[Normalize] Original Peak: {orig_peak_db:.2f} dBFS\")\n",
        "    # Use global TARGET_PEAK_DB instead of cfg.TARGET_PEAK_DB\n",
        "    if orig_peak > 0 and TARGET_PEAK_DB is not None:\n",
        "        scale = (10 ** (TARGET_PEAK_DB / 20)) / orig_peak\n",
        "        y_norm = y * scale\n",
        "        final_peak_db = TARGET_PEAK_DB\n",
        "    else:\n",
        "        y_norm = y\n",
        "        final_peak_db = orig_peak_db\n",
        "    log_message_func(f\"[Normalize] Final Peak (peak-only): {final_peak_db:.2f} dBFS\")\n",
        "    return y_norm, {\n",
        "        'original_lufs':   None,\n",
        "        'original_peak':   orig_peak_db,\n",
        "        'normalized_lufs': None,\n",
        "        'normalized_peak': final_peak_db,\n",
        "        'method':          'peak_only',\n",
        "        'duration':        len(y_norm) / sr\n",
        "    }\n",
        "\n",
        "def normalize_by_peak_only(y: np.ndarray, sr: int, log_message_func) -> tuple:\n",
        "    \"\"\"\n",
        "    Peak-only normalization fallback.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.ndarray\n",
        "    sr : int\n",
        "    log_message_func : callable\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        y_norm : np.ndarray\n",
        "        report : dict\n",
        "    \"\"\"\n",
        "    log_message_func(\"[Normalize] Applying peak-only normalization\")\n",
        "    orig_peak = np.max(np.abs(y))\n",
        "    orig_peak_db = 20 * np.log10(orig_peak) if orig_peak > 0 else -np.inf\n",
        "    log_message_func(f\"[Normalize] Original Peak: {orig_peak_db:.2f} dBFS\")\n",
        "    if orig_peak > 0 and cfg.TARGET_PEAK_DB is not None:\n",
        "        scale = (10 ** (cfg.TARGET_PEAK_DB / 20)) / orig_peak\n",
        "        y_norm = y * scale\n",
        "        final_peak_db = cfg.TARGET_PEAK_DB\n",
        "    else:\n",
        "        y_norm = y\n",
        "        final_peak_db = orig_peak_db\n",
        "    log_message_func(f\"[Normalize] Final Peak (peak-only): {final_peak_db:.2f} dBFS\")\n",
        "    return y_norm, {\n",
        "        'original_lufs':   None,\n",
        "        'original_peak':   orig_peak_db,\n",
        "        'normalized_lufs': None,\n",
        "        'normalized_peak': final_peak_db,\n",
        "        'method':          'peak_only',\n",
        "        'duration':        len(y_norm) / sr\n",
        "    }\n",
        "\n",
        "def pan_percent(left: np.ndarray, right: np.ndarray) -> tuple:\n",
        "    \"\"\"\n",
        "    Compute percent power in left/right channels and deviation from center.\n",
        "    \"\"\"\n",
        "    power_left = np.sum(left ** 2)\n",
        "    power_right = np.sum(right ** 2)\n",
        "    total = power_left + power_right\n",
        "    if total < 1e-10:\n",
        "        return 50.0, 50.0, 0.0\n",
        "    left_pct = (power_left / total) * 100\n",
        "    right_pct = (power_right / total) * 100\n",
        "    return left_pct, right_pct, abs(left_pct - 50)\n",
        "\n",
        "def normalize_panning(audio: np.ndarray, log_message_func) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply corrective gain to right channel to match left RMS.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio : np.ndarray\n",
        "    log_message_func : callable\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "    \"\"\"\n",
        "    if audio.ndim == 1:\n",
        "        log_message_func(\"[Panning] Converting mono to stereo for correction\")\n",
        "        audio = np.stack([audio, audio], axis=0)\n",
        "    left, right = audio[0], audio[1]\n",
        "    rms_left = np.sqrt(np.mean(left ** 2))\n",
        "    rms_right = np.sqrt(np.mean(right ** 2))\n",
        "    if rms_right < 1e-6:\n",
        "        return audio\n",
        "    log_message_func(\"[Panning] Applying panning correction\")\n",
        "    return np.vstack((left, right * (rms_left / rms_right)))\n",
        "\n",
        "def dbfs(amplitude: float) -> float:\n",
        "    \"\"\"\n",
        "    Convert linear amplitude to dBFS.\n",
        "    \"\"\"\n",
        "    return 20 * np.log10(amplitude) if amplitude > 0 else -np.inf\n",
        "\n",
        "def soft_limiter(signal: np.ndarray, threshold_db: float=-3.00, ratio: float=10) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Soft clip any samples above threshold with given ratio.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : np.ndarray\n",
        "    threshold_db : float\n",
        "    ratio : float\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "    \"\"\"\n",
        "    threshold_lin = 10 ** (threshold_db / 20)\n",
        "    if signal.ndim > 1:\n",
        "        limited = np.zeros_like(signal)\n",
        "        for c in range(signal.shape[0]):\n",
        "            ch = signal[c]\n",
        "            abs_ch = np.abs(ch)\n",
        "            above = abs_ch > threshold_lin\n",
        "            limited[c, ~above] = ch[~above]\n",
        "            limited[c, above] = np.sign(ch[above]) * threshold_lin * ((abs_ch[above] / threshold_lin) ** (1/ratio))\n",
        "        return limited\n",
        "    else:\n",
        "        abs_sig = np.abs(signal)\n",
        "        limited = signal.copy()\n",
        "        above = abs_sig > threshold_lin\n",
        "        limited[above] = np.sign(signal[above]) * threshold_lin * ((abs_sig[above] / threshold_lin) ** (1/ratio))\n",
        "        return limited\n",
        "\n",
        "def format_duration(seconds: float) -> str:\n",
        "    \"\"\"\n",
        "    Format a duration into the current time_unit.\n",
        "    \"\"\"\n",
        "    if time_unit == \"Milliseconds\":\n",
        "        ms = seconds * 1000\n",
        "        return f\"{ms:.2f} ms\" if ms < 1000 else f\"{seconds:.4f} s\"\n",
        "    if time_unit == \"Seconds\":\n",
        "        return f\"{seconds:.4f} s\"\n",
        "    if time_unit == \"Minutes\":\n",
        "        return f\"{seconds/60:.4f} min\"\n",
        "    if time_unit == \"Hours\":\n",
        "        return f\"{seconds/3600:.4f} hours\"\n",
        "    return f\"{seconds:.4f} s\"\n",
        "\n",
        "def detect_clipping(y: np.ndarray, use_cuda: bool, device) -> tuple:\n",
        "    \"\"\"\n",
        "    Detect sample clipping beyond ±0.999.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (clipped: bool, ratio: float)\n",
        "    \"\"\"\n",
        "    if use_cuda:\n",
        "        try:\n",
        "            t = torch.tensor(y, device=device)\n",
        "            mask = torch.abs(t) >= 0.999\n",
        "            ratio = mask.float().mean().item()\n",
        "            return ratio > 0.001, ratio\n",
        "        except RuntimeError:\n",
        "            mask = np.abs(y) >= 0.999\n",
        "            ratio = mask.sum() / y.size\n",
        "            return ratio > 0.001, ratio\n",
        "    else:\n",
        "        mask = np.abs(y) >= 0.999\n",
        "        ratio = mask.sum() / y.size\n",
        "        return ratio > 0.001, ratio\n",
        "\n",
        "def attenuate_audio(y: np.ndarray, use_cuda: bool, device) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Scale down waveform to avoid clipping, leaving 0.05 headroom.\n",
        "    \"\"\"\n",
        "    if use_cuda:\n",
        "        try:\n",
        "            t = torch.tensor(y, device=device)\n",
        "            peak = torch.max(torch.abs(t)).item()\n",
        "            if peak == 0: return y\n",
        "            factor = min(1.0 - 0.05, 1.0 / peak)\n",
        "            return (t * factor).cpu().numpy()\n",
        "        except RuntimeError:\n",
        "            peak = np.max(np.abs(y))\n",
        "            if peak == 0: return y\n",
        "            factor = min(1.0 - 0.05, 1.0 / peak)\n",
        "            return y * factor\n",
        "    else:\n",
        "        peak = np.max(np.abs(y))\n",
        "        if peak == 0: return y\n",
        "        factor = min(1.0 - 0.05, 1.0 / peak)\n",
        "        return y * factor\n",
        "\n",
        "def calculate_adaptive_hop_length(length: int) -> int:\n",
        "    \"\"\"\n",
        "    Determine hop length for RMS frames based on total length.\n",
        "    \"\"\"\n",
        "    return min(DEFAULT_HOP_LENGTH, max(32, length // MIN_FRAMES_FOR_RMS))\n",
        "\n",
        "def auto_slice_audio(y: np.ndarray, sr: int, use_cuda: bool, device, log_message_func) -> tuple:\n",
        "    \"\"\"\n",
        "    Rough silence trimming via frame-based RMS threshold.\n",
        "    \"\"\"\n",
        "    L = len(y)\n",
        "    if L < 128: return y, 0, L\n",
        "    hop = calculate_adaptive_hop_length(L)\n",
        "    frame_length = min(2048, L)\n",
        "    if use_cuda:\n",
        "        try:\n",
        "            t = torch.tensor(y, device=device)\n",
        "            frames = t.unfold(0, frame_length, hop)\n",
        "            rms = torch.sqrt((frames ** 2).mean(dim=1))\n",
        "            db = 20 * torch.log10(rms / rms.max() + 1e-7)\n",
        "            db = db.cpu().numpy()\n",
        "        except RuntimeError:\n",
        "            log_message_func(\"[Silence] GPU OOM; falling back to CPU RMS\")\n",
        "            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop)[0]\n",
        "            db = librosa.amplitude_to_db(rms, ref=np.max)\n",
        "    else:\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop)[0]\n",
        "        db = librosa.amplitude_to_db(rms, ref=np.max)\n",
        "    idx = np.where(db > DB_THRESH)[0]\n",
        "    if idx.size == 0: return y, 0, L\n",
        "    start = idx[0] * hop\n",
        "    end = min(L, (idx[-1] + 1) * hop)\n",
        "    return y[start:end], start, end\n",
        "\n",
        "def slice_edge_silence(y: np.ndarray, thresh: float=EDGE_SILENCE_THRESHOLD) -> tuple:\n",
        "    \"\"\"\n",
        "    Precise trim of very low-level edge silence.\n",
        "    \"\"\"\n",
        "    abs_y = np.abs(y)\n",
        "    nz = np.where(abs_y > thresh)[0]\n",
        "    if nz.size == 0: return y, 0, len(y)\n",
        "    return y[nz[0]: nz[-1] + 1], nz[0], nz[-1] + 1\n",
        "\n",
        "def hard_slice_to_zero(y: np.ndarray, thresh: float=EDGE_SILENCE_THRESHOLD) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Zero out samples before/after threshold crossings.\n",
        "    \"\"\"\n",
        "    abs_y = np.abs(y)\n",
        "    if not (abs_y > thresh).any(): return y\n",
        "    s = np.argmax(abs_y > thresh)\n",
        "    e = len(y) - np.argmax(abs_y[::-1] > thresh)\n",
        "    y[:s] = 0; y[e:] = 0\n",
        "    return y\n",
        "\n",
        "def process_silence(y: np.ndarray, sr: int, use_cuda: bool, device, log_message_func) -> tuple:\n",
        "    \"\"\"\n",
        "    Full two-stage silence trimming pipeline.\n",
        "    \"\"\"\n",
        "    y_mono = librosa.to_mono(y) if y.ndim > 1 else y\n",
        "    tmono, s0, e0 = auto_slice_audio(y_mono, sr, use_cuda, device, log_message_func)\n",
        "    trimmed = (y[:, s0:e0] if y.ndim > 1 else y[s0:e0])\n",
        "    fine_m, fs, fe = slice_edge_silence(tmono)\n",
        "    fine = (trimmed[:, fs:fe] if y.ndim > 1 else trimmed[fs:fe])\n",
        "    if fine.ndim > 1:\n",
        "        proc = fine.copy()\n",
        "        for c in range(proc.shape[0]):\n",
        "            proc[c] = hard_slice_to_zero(fine[c], thresh=EDGE_SILENCE_THRESHOLD)\n",
        "    else:\n",
        "        proc = hard_slice_to_zero(fine.copy(), thresh=EDGE_SILENCE_THRESHOLD)\n",
        "    proc[np.abs(proc) < 1e-6] = 0\n",
        "    start = s0 + fs\n",
        "    end = start + (fe - fs)\n",
        "    pre = start / sr\n",
        "    post = (len(y_mono) - end) / sr\n",
        "    total = pre + post\n",
        "    return proc, (pre, post, total), (start, end)\n",
        "\n",
        "def plot_trim_boundaries(y: np.ndarray, sr: int, s0: int, e0: int, segments=None):\n",
        "    \"\"\"\n",
        "    Plot waveform with trim boundaries (and optional segment markers).\n",
        "    \"\"\"\n",
        "    if not cfg.visualize: return\n",
        "    unit_dict = {\"Milliseconds\":(\"ms\",1000),\"Seconds\":(\"s\",1),\n",
        "                 \"Minutes\":(\"min\",1/60),\"Hours\":(\"hours\",1/3600)}\n",
        "    unit, fac = unit_dict.get(time_unit, (\"s\",1))\n",
        "    times = np.arange(y.shape[-1]) / sr * fac\n",
        "    plt.figure(figsize=(14,3))\n",
        "    plt.plot(times, y.T if y.ndim>1 else y, alpha=0.8)\n",
        "    plt.axvline(s0/sr*fac, linestyle='--', color='red', label='Trim Boundary')\n",
        "    plt.axvline(e0/sr*fac, linestyle='--', color='red')\n",
        "    if segments:\n",
        "        for ss, ee in segments:\n",
        "            plt.axvline(ss/sr*fac, linestyle='-', color='green', alpha=0.7)\n",
        "            plt.axvline(ee/sr*fac, linestyle='-', color='green', alpha=0.7)\n",
        "        plt.title(f\"Waveform with Trim Boundaries (Red) and Segments (Green) ({time_unit})\")\n",
        "    else:\n",
        "        plt.title(f\"Waveform with Trim Boundaries (Red) ({time_unit})\")\n",
        "    plt.xlabel(f\"Time ({unit})\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_zoomed_silence(y: np.ndarray, sr: int, s0: int, e0: int, zoom: float=0.05):\n",
        "    \"\"\"\n",
        "    Plot a zoomed-in view of the silence before/after trim.\n",
        "    \"\"\"\n",
        "    if not cfg.visualize: return\n",
        "    zs = int(sr * zoom)\n",
        "    unit_dict = {\"Milliseconds\":(\"ms\",1000),\"Seconds\":(\"s\",1),\n",
        "                 \"Minutes\":(\"min\",1/60),\"Hours\":(\"hours\",1/3600)}\n",
        "    unit, fac = unit_dict.get(time_unit, (\"s\",1))\n",
        "    length = y.shape[-1] if y.ndim>1 else y.size\n",
        "    if s0 > zs:\n",
        "        pre = y[:,s0-zs:s0] if y.ndim>1 else y[s0-zs:s0]\n",
        "        t0 = np.linspace(-zoom*fac,0,pre.shape[-1])\n",
        "    else:\n",
        "        pre = y[:s0] if y.ndim>1 else y[:s0]\n",
        "        t0 = np.linspace(-pre.shape[-1]/sr*fac,0,pre.shape[-1])\n",
        "    if e0+zs < length:\n",
        "        post = y[:,e0:e0+zs] if y.ndim>1 else y[e0:e0+zs]\n",
        "        t1 = np.linspace(0,zoom*fac,post.shape[-1])\n",
        "    else:\n",
        "        post = y[:,e0:] if y.ndim>1 else y[e0:]\n",
        "        t1 = np.linspace(0,post.shape[-1]/sr*fac,post.shape[-1])\n",
        "    fig, axs = plt.subplots(2,1,figsize=(14,4))\n",
        "    if pre.shape[-1] > 0:\n",
        "        axs[0].plot(t0, pre.T if pre.ndim>1 else pre)\n",
        "        axs[0].set_xlim(t0[0], t0[-1])\n",
        "    else:\n",
        "        axs[0].text(0.5,0.5,\"No silence before trim\",ha='center',va='center')\n",
        "        axs[0].set_xlim(-1,1)\n",
        "    axs[0].set_title(f\"Zoomed Silence Before Trim ({time_unit})\")\n",
        "    axs[0].set_xlabel(f\"Time ({unit})\")\n",
        "    if post.shape[-1] > 0:\n",
        "        axs[1].plot(t1, post.T if post.ndim>1 else post)\n",
        "        axs[1].set_xlim(t1[0], t1[-1])\n",
        "    else:\n",
        "        axs[1].text(0.5,0.5,\"No silence after trim\",ha='center',va='center')\n",
        "        axs[1].set_xlim(-1,1)\n",
        "    axs[1].set_title(f\"Zoomed Silence After Trim ({time_unit})\")\n",
        "    axs[1].set_xlabel(f\"Time ({unit})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_normalized_waveform(y: np.ndarray, sr: int, title: str=\"Normalized Waveform\"):\n",
        "    \"\"\"\n",
        "    Plot the final processed or normalized waveform.\n",
        "    \"\"\"\n",
        "    if not cfg.visualize: return\n",
        "    length = y.shape[-1] if y.ndim>1 else y.size\n",
        "    if length == 0: return\n",
        "    unit_dict = {\"Milliseconds\":(\"ms\",1000),\"Seconds\":(\"s\",1),\n",
        "                 \"Minutes\":(\"min\",1/60),\"Hours\":(\"hours\",1/3600)}\n",
        "    unit, fac = unit_dict.get(time_unit, (\"s\",1))\n",
        "    times = np.arange(length) / sr * fac\n",
        "    plt.figure(figsize=(14,3))\n",
        "    if y.ndim>1:\n",
        "        for c in range(y.shape[0]):\n",
        "            plt.plot(times, y[c], alpha=0.7, label=f'Channel {c+1}')\n",
        "        plt.legend()\n",
        "    else:\n",
        "        plt.plot(times, y, alpha=0.8)\n",
        "    plt.title(f\"{title} ({time_unit})\")\n",
        "    plt.xlabel(f\"Time ({unit})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_all_audio_files(path: str) -> list:\n",
        "    \"\"\"\n",
        "    Recursively collect all valid audio files under path.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    if os.path.isfile(path) and path.lower().endswith(VALID_FORMATS):\n",
        "        files.append(path)\n",
        "    else:\n",
        "        for root, _, fnames in os.walk(path):\n",
        "            for f in fnames:\n",
        "                if f.lower().endswith(VALID_FORMATS):\n",
        "                    files.append(os.path.join(root, f))\n",
        "    return files\n",
        "\n",
        "def get_export_path(file_path: str, input_dir: str, is_segmented: bool=False) -> str:\n",
        "    \"\"\"\n",
        "    Compute and create export directory for a given file.\n",
        "    \"\"\"\n",
        "    if os.path.isdir(input_dir):\n",
        "        # Get relative path while preserving directory structure\n",
        "        rel_path = os.path.relpath(file_path, input_dir)\n",
        "        # Remove filename to get directory path\n",
        "        rel_dir = os.path.dirname(rel_path)\n",
        "        base_path = os.path.join(export, rel_dir)\n",
        "    else:\n",
        "        # For single file input, use parent directory structure\n",
        "        parent_dir = os.path.dirname(input_dir)\n",
        "        rel_path = os.path.relpath(file_path, parent_dir)\n",
        "        rel_dir = os.path.dirname(rel_path)\n",
        "        base_path = os.path.join(export, rel_dir)\n",
        "\n",
        "    if is_segmented:\n",
        "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        base_path = os.path.join(base_path, f\"{file_name}_segments\")\n",
        "\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    return base_path\n",
        "\n",
        "def export_audio(y: np.ndarray, sr: int, orig: str, fmt: str, idx: int=None) -> str:\n",
        "    \"\"\"\n",
        "    Write processed audio to disk in the desired format.\n",
        "    \"\"\"\n",
        "    is_segmented = idx is not None\n",
        "    export_path = get_export_path(orig, input_path, is_segmented)\n",
        "    base = os.path.splitext(os.path.basename(orig))[0]\n",
        "    if cfg.channels == \"mono\" and y.ndim > 1:\n",
        "        y = y.mean(axis=0)\n",
        "    elif cfg.channels == \"stereo\" and y.ndim == 1:\n",
        "        y = np.stack([y, y], axis=0)\n",
        "    name = f\"{base}_segment_{idx+1}.{fmt}\" if is_segmented else f\"{base}.{fmt}\"\n",
        "    out_path = os.path.join(export_path, name)\n",
        "    if fmt.lower() == \"mp3\":\n",
        "        tmp = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "        tmp_path = tmp.name\n",
        "        tmp.close()\n",
        "        sf.write(tmp_path, y.T if y.ndim>1 else y, sr, subtype='FLOAT')\n",
        "        try:\n",
        "            cmd = [\n",
        "                \"ffmpeg\", \"-y\", \"-i\", tmp_path,\n",
        "                \"-c:a\", \"libmp3lame\",\n",
        "                \"-b:a\", cfg.mp3_bitrate,\n",
        "                \"-ar\", str(sr),\n",
        "                out_path\n",
        "            ]\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "    else:\n",
        "        actual_bit_depth = cfg.bit_depth\n",
        "        if fmt == \"flac\" and cfg.bit_depth == \"32\":\n",
        "            print(\"[Warning] 32-bit FLAC not supported; switching to 24-bit.\")\n",
        "            actual_bit_depth = \"24\"\n",
        "        subtype = f\"PCM_{actual_bit_depth}\"\n",
        "        sf.write(out_path, y.T if y.ndim>1 else y, sr, subtype=subtype)\n",
        "    return out_path\n",
        "\n",
        "def process_file(file: str, params: dict) -> tuple:\n",
        "    \"\"\"\n",
        "    End-to-end processing of a single audio file, including trimming,\n",
        "    normalization, segmentation, panning, and export.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        (file_path, results_dict)\n",
        "    \"\"\"\n",
        "    file_results = {\n",
        "        'messages': [],\n",
        "        'export_paths': [],\n",
        "        'plot_data': {},\n",
        "        'total_silence': 0.0,\n",
        "        'norm_reports': [],\n",
        "        'original_metrics': None\n",
        "    }\n",
        "    def log_message(msg):\n",
        "        file_results['messages'].append(msg)\n",
        "    try:\n",
        "        use_cuda = params['use_cuda']\n",
        "        device = params['device']\n",
        "        sample_rate_val = int(params['sample_rate'])\n",
        "        current_segmentation = params['segmentation']\n",
        "        duration_seconds = params['duration_seconds']\n",
        "        time_unit_local = params['time_unit']\n",
        "        normalize_audio_flag = params['normalize_audio']\n",
        "        panning_correction_flag = params['panning_correction']\n",
        "        apply_silence_trimming_flag = params['apply_silence_trimming']\n",
        "\n",
        "        log_message(f\"[Info] Processing File: {file}\")\n",
        "        log_message(f\"[Info] Processing Device: {'GPU' if use_cuda else 'CPU'}\")\n",
        "        log_message(f\"[Info] Silence Trimming: {'Enabled' if apply_silence_trimming_flag else 'Disabled'}\")\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(file, sr=sample_rate_val, mono=False)\n",
        "        except Exception as e:\n",
        "            log_message(f\"[Error] Could not load audio: {e}\")\n",
        "            y, sr = librosa.load(file, sr=sample_rate_val, mono=True)\n",
        "\n",
        "        log_message(\"\")  # Blank line\n",
        "        try:\n",
        "            info = sf.info(file)\n",
        "            log_message(\"[Info] Input Audio Details:\")\n",
        "            log_message(f\"  Path: {file}\")\n",
        "            log_message(f\"  Format: {os.path.splitext(file)[1][1:].upper()}\")\n",
        "            log_message(f\"  Sample Rate: {info.samplerate} Hz\")\n",
        "            log_message(f\"  Bit Depth: {info.subtype}\")\n",
        "            log_message(f\"  Channels: {info.channels}\")\n",
        "            log_message(f\"  Duration: {format_duration(info.duration)}\")\n",
        "            if y.ndim > 1 and y.shape[0] == 2:\n",
        "                lp, rp, _ = pan_percent(y[0], y[1])\n",
        "                log_message(f\"  Pan Balance Input: Left = {lp:.2f}%, Right = {rp:.2f}%\")\n",
        "        except Exception as e:\n",
        "            log_message(f\"[Error] Could not read metadata: {e}\")\n",
        "\n",
        "        original_channels = \"mono\" if y.ndim == 1 else \"stereo\"\n",
        "        log_message(f\"[Info] Original audio channels: {original_channels}\")\n",
        "\n",
        "        if cfg.channels == \"mono\" and y.ndim > 1:\n",
        "            log_message(\"[Info] Converting stereo → mono\")\n",
        "            y = librosa.to_mono(y)\n",
        "        elif cfg.channels == \"stereo\" and y.ndim == 1:\n",
        "            log_message(\"[Info] Converting mono → stereo\")\n",
        "            y = np.stack([y, y], axis=0)\n",
        "\n",
        "        if y.ndim > 1 and y.shape[0] == 2:\n",
        "            lp, rp, _ = pan_percent(y[0], y[1])\n",
        "            log_message(f\"[Info] Pan After Conversion: Left = {lp:.2f}%, Right = {rp:.2f}%\")\n",
        "\n",
        "        if panning_correction_flag:\n",
        "            y = normalize_panning(y, log_message)\n",
        "            if cfg.channels == \"mono\":\n",
        "                y = librosa.to_mono(y)\n",
        "                log_message(\"[Panning] Converted corrected stereo back to mono\")\n",
        "            else:\n",
        "                lp, rp, _ = pan_percent(y[0], y[1])\n",
        "                log_message(f\"[Panning] Corrected Pan Balance: Left = {lp:.2f}%, Right = {rp:.2f}%\")\n",
        "\n",
        "        clipped, ratio = detect_clipping(y, use_cuda, device)\n",
        "        if clipped:\n",
        "            log_message(f\"[Clipping] Detected {ratio:.2%} clipped → attenuating\")\n",
        "            y = attenuate_audio(y, use_cuda, device)\n",
        "\n",
        "        # Apply silence trimming only if enabled\n",
        "        if apply_silence_trimming_flag:\n",
        "            y_proc, (pre, post, total_silence), (s_start, s_end) = process_silence(\n",
        "                y, sr, use_cuda, device, log_message\n",
        "            )\n",
        "            log_message(\"\")\n",
        "            log_message(\"[Silence] Trim Results:\")\n",
        "            log_message(f\"  Start silence: {format_duration(pre)}\")\n",
        "            log_message(f\"  End silence: {format_duration(post)}\")\n",
        "            log_message(f\"  Total silence removed: {format_duration(total_silence)}\")\n",
        "            file_results['total_silence'] = total_silence\n",
        "        else:\n",
        "            y_proc = y\n",
        "            s_start = 0\n",
        "            s_end = len(y)\n",
        "            file_results['total_silence'] = 0.0\n",
        "            log_message(\"[Silence] Skipping silence trimming\")\n",
        "\n",
        "        if not normalize_audio_flag:\n",
        "            try:\n",
        "                log_message(\"[Info] Measuring original loudness\")\n",
        "                orig_metrics = measure_loudness(y_proc, sr)\n",
        "                file_results['original_metrics'] = orig_metrics\n",
        "                if orig_metrics['lufs'] is not None:\n",
        "                    log_message(f\"  Original LUFS: {orig_metrics['lufs']:.2f} LUFS\")\n",
        "                log_message(f\"  Original Peak: {orig_metrics['peak']:.2f} dBFS\")\n",
        "            except Exception as e:\n",
        "                log_message(f\"[Error] Loudness measure failed: {e}\")\n",
        "\n",
        "        norm_report = None\n",
        "        if normalize_audio_flag and not current_segmentation:\n",
        "            log_message(\"\")\n",
        "            log_message(\"[Normalize] Starting Loudness Normalization (full)\")\n",
        "            try:\n",
        "                y_proc, norm_report = normalize_loudness_true(y_proc, sr, log_message)\n",
        "                log_message(\"[Normalize] Completed full normalization\")\n",
        "            except Exception as e:\n",
        "                log_message(f\"[Normalize] Failed: {e}\")\n",
        "        file_results['norm_reports'].append(norm_report)\n",
        "\n",
        "        processed_duration = y_proc.shape[-1] / sr\n",
        "        export_paths = []\n",
        "        segments = []\n",
        "        seg_failed = False\n",
        "\n",
        "        if current_segmentation:\n",
        "            log_message(f\"[Segmentation] Segment duration: {format_duration(duration_seconds)}\")\n",
        "            log_message(f\"[Segmentation] Processed duration: {format_duration(processed_duration)}\")\n",
        "\n",
        "            if processed_duration < duration_seconds:\n",
        "                log_message(\"[Error] Audio shorter than segment duration → skipping segmentation\")\n",
        "                seg_failed = True\n",
        "                current_segmentation = False\n",
        "            else:\n",
        "                seg_len = int(sr * duration_seconds)\n",
        "                total_len = y_proc.shape[-1]\n",
        "                n_full = total_len // seg_len\n",
        "                for i in range(n_full):\n",
        "                    segments.append((i*seg_len, (i+1)*seg_len))\n",
        "                rem = total_len % seg_len\n",
        "                if rem > sr:\n",
        "                    segments.append((n_full*seg_len, total_len))\n",
        "                    log_message(f\"[Segmentation] Final short segment: {format_duration(rem/sr)}\")\n",
        "                elif rem > 0:\n",
        "                    log_message(f\"[Segmentation] Skipping tiny remainder ({format_duration(rem/sr)})\")\n",
        "                log_message(\"\")\n",
        "                log_message(\"[Segmentation] Results:\")\n",
        "                log_message(f\"  Total segments: {len(segments)}\")\n",
        "                log_message(\"\")\n",
        "                log_message(\"[Summary] Processing:\")\n",
        "                log_message(f\"  Total silence removed: {format_duration(file_results['total_silence'])}\")\n",
        "                log_message(f\"  Total segments: {len(segments)}\")\n",
        "        else:\n",
        "            log_message(\"\")\n",
        "            log_message(\"[Summary] Processing:\")\n",
        "            log_message(f\"  Total silence removed: {format_duration(file_results['total_silence'])}\")\n",
        "            log_message(\"  Exporting single file\")\n",
        "\n",
        "        file_results['plot_data'] = {\n",
        "            'y': y,\n",
        "            'y_proc': y_proc,\n",
        "            'sr': sr,\n",
        "            'samp_start': s_start,\n",
        "            'samp_end': s_end,\n",
        "            'segments': segments if segments else None,\n",
        "            'segmentation_failed': seg_failed,\n",
        "            'current_segmentation': current_segmentation,\n",
        "            'total_len': y_proc.shape[-1] if y_proc is not None else 0\n",
        "        }\n",
        "\n",
        "        if current_segmentation and segments and not seg_failed:\n",
        "            for i, (st, en) in enumerate(segments):\n",
        "                seg_audio = y_proc[:, st:en] if y_proc.ndim > 1 else y_proc[st:en]\n",
        "                seg_report = None\n",
        "                if normalize_audio_flag:\n",
        "                    log_message(\"\")\n",
        "                    log_message(f\"[Normalize] Segment {i+1} normalization\")\n",
        "                    try:\n",
        "                        seg_audio, seg_report = normalize_loudness_true(seg_audio, sr, log_message)\n",
        "                        log_message(f\"[Normalize] Completed segment {i+1}\")\n",
        "                    except Exception as e:\n",
        "                        log_message(f\"[Normalize] Segment {i+1} failed: {e}\")\n",
        "                file_results['norm_reports'].append(seg_report)\n",
        "                path = export_audio(seg_audio, sr, file, out_format, i)\n",
        "                export_paths.append(path)\n",
        "                file_results['export_paths'].append({\n",
        "                    'path': path,\n",
        "                    'seg_index': i,\n",
        "                    'seg_duration': seg_audio.shape[-1]/sr,\n",
        "                    'norm_report': seg_report,\n",
        "                    'original_metrics': file_results['original_metrics']\n",
        "                })\n",
        "        elif not seg_failed:\n",
        "            path = export_audio(y_proc, sr, file, out_format)\n",
        "            export_paths.append(path)\n",
        "            file_results['export_paths'].append({\n",
        "                'path': path,\n",
        "                'seg_index': None,\n",
        "                'seg_duration': processed_duration,\n",
        "                'norm_report': norm_report,\n",
        "                'original_metrics': file_results['original_metrics']\n",
        "            })\n",
        "        else:\n",
        "            log_message(\"[Error] Export skipped due to segmentation failure\")\n",
        "\n",
        "        return file, file_results\n",
        "\n",
        "    except Exception as e:\n",
        "        file_results['messages'].append(f\"[Error] Unexpected failure: {e}\")\n",
        "        return file, file_results\n",
        "\n",
        "def run_tests():\n",
        "    import numpy as np\n",
        "    print(\"=== Running Basic Functional Tests ===\")\n",
        "    sr = 48000\n",
        "    tone = 0.5 * np.sin(2 * np.pi * 440 * np.linspace(0,1,sr))\n",
        "    silence = np.zeros(int(0.5 * sr))\n",
        "    y = np.concatenate([silence, tone, silence]).astype(np.float32)\n",
        "\n",
        "    y_trim, (pre,post,total), (s0,e0) = process_silence(y, sr, cfg.use_cuda, cfg.device, print)\n",
        "    print(f\"Trim results: pre={pre:.3f}s, post={post:.3f}s, total={total:.3f}s\")\n",
        "\n",
        "    stereo = np.vstack([y,y])\n",
        "    lp,rp,dev = pan_percent(stereo[0], stereo[1])\n",
        "    print(f\"Pan percent: L={lp:.1f}%, R={rp:.1f}%, dev={dev:.1f}%\")\n",
        "\n",
        "    global TARGET_LUFS, TARGET_PEAK_DB\n",
        "    sl,sp = cfg.TARGET_LUFS, cfg.TARGET_PEAK_DB\n",
        "    TARGET_LUFS = -14.0; TARGET_PEAK_DB = -3.0\n",
        "    try:\n",
        "        y_norm, report = normalize_loudness_true(y_trim, sr, print)\n",
        "        print(f\"Norm report: {report}\")\n",
        "    finally:\n",
        "        TARGET_LUFS, TARGET_PEAK_DB = sl, sp\n",
        "\n",
        "    print(\"Basic functional tests completed.\")\n",
        "\n",
        "def main():\n",
        "    tqdm.write(f\"[Info] Export path: {export}\")\n",
        "    files = get_all_audio_files(input_path)\n",
        "    tqdm.write(f\"[Info] Processing {len(files)} audio file(s)\")\n",
        "\n",
        "    params = {\n",
        "        'sample_rate': sample_rate,\n",
        "        'segmentation': cfg.segmentation,\n",
        "        'time_unit': time_unit,\n",
        "        'duration': duration,\n",
        "        'normalize_audio': normalize_audio,\n",
        "        'panning_correction': cfg.panning_correction,\n",
        "        'duration_seconds': cfg.duration_seconds,\n",
        "        'use_cuda': cfg.use_cuda,\n",
        "        'device': cfg.device,\n",
        "        'apply_silence_trimming': cfg.apply_silence_trimming\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    failed_files = []\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        future_to_file = {executor.submit(process_file, f, params): f for f in files}\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_file),\n",
        "                           total=len(files), desc=\"Processing files\"):\n",
        "            f = future_to_file[future]\n",
        "            try:\n",
        "                file, file_results = future.result()\n",
        "                results[file] = file_results\n",
        "            except Exception as e:\n",
        "                tqdm.write(f\"[Error] Processing {f} failed: {e}\")\n",
        "                failed_files.append(f)\n",
        "\n",
        "    any_segmentation_failed = False\n",
        "\n",
        "    for file, file_results in results.items():\n",
        "        for msg in file_results['messages']:\n",
        "            tqdm.write(msg)\n",
        "\n",
        "        plot_data = file_results['plot_data']\n",
        "        export_info = file_results['export_paths']\n",
        "\n",
        "        if plot_data['segmentation_failed']:\n",
        "            any_segmentation_failed = True\n",
        "\n",
        "        if not plot_data['segmentation_failed']:\n",
        "            if plot_data['current_segmentation'] and plot_data['segments']:\n",
        "                plot_trim_boundaries(\n",
        "                    plot_data['y_proc'],\n",
        "                    plot_data['sr'],\n",
        "                    0,\n",
        "                    plot_data['total_len'],\n",
        "                    plot_data['segments']\n",
        "                )\n",
        "            else:\n",
        "                plot_trim_boundaries(\n",
        "                    plot_data['y'],\n",
        "                    plot_data['sr'],\n",
        "                    plot_data['samp_start'],\n",
        "                    plot_data['samp_end']\n",
        "                )\n",
        "\n",
        "            plot_zoomed_silence(\n",
        "                plot_data['y'],\n",
        "                plot_data['sr'],\n",
        "                plot_data['samp_start'],\n",
        "                plot_data['samp_end']\n",
        "            )\n",
        "\n",
        "            title = \"Normalized Waveform\" if normalize_audio else \"Processed Waveform (without normalization)\"\n",
        "            plot_normalized_waveform(plot_data['y_proc'], plot_data['sr'], title)\n",
        "\n",
        "        total_segments_count = (\n",
        "            len(export_info) if cfg.segmentation and not plot_data['segmentation_failed']\n",
        "            else (1 if export_info else 0)\n",
        "        )\n",
        "\n",
        "        for export_item in export_info:\n",
        "            tqdm.write(\"\")  # Blank line before Exported Audio Details\n",
        "            if export_item['seg_index'] is not None:\n",
        "                tqdm.write(\"Exported Segment Details:\")\n",
        "            else:\n",
        "                tqdm.write(\"Exported Audio Details:\")\n",
        "\n",
        "            tqdm.write(f\"  Path: {export_item['path']}\")\n",
        "            tqdm.write(f\"  Format: {out_format.upper()}\")\n",
        "\n",
        "            try:\n",
        "                info_exp = sf.info(export_item['path'])\n",
        "                tqdm.write(f\"  Sample Rate: {info_exp.samplerate} Hz\")\n",
        "                tqdm.write(f\"  Bit Depth: {info_exp.subtype}\")\n",
        "                channels_exp = info_exp.channels\n",
        "                ch_str = \"Mono\" if channels_exp == 1 else \"Stereo\"\n",
        "                tqdm.write(f\"  Channels: {ch_str}\")\n",
        "            except Exception as e:\n",
        "                tqdm.write(f\"  [Error] Could not read exported file metadata: {e}\")\n",
        "\n",
        "            if out_format.lower() == \"mp3\":\n",
        "                tqdm.write(f\"  MP3 Bitrate: {cfg.mp3_bitrate}\")\n",
        "\n",
        "            if cfg.segmentation and not plot_data['segmentation_failed']:\n",
        "                tqdm.write(f\"  Total Segments: {total_segments_count}\")\n",
        "\n",
        "            if cfg.segmentation:\n",
        "                tqdm.write(f\"  Time Unit: {time_unit}\")\n",
        "\n",
        "            tqdm.write(f\"  Duration: {format_duration(export_item['seg_duration'])}\")\n",
        "\n",
        "            try:\n",
        "                y_exp, sr_exp = librosa.load(export_item['path'], sr=None, mono=False)\n",
        "                if y_exp.ndim > 1 and y_exp.shape[0] == 2:\n",
        "                    l_pct_out, r_pct_out, _ = pan_percent(y_exp[0], y_exp[1])\n",
        "                    tqdm.write(f\"  Pan Balance Output: Left = {l_pct_out:.2f}%, Right = {r_pct_out:.2f}%\")\n",
        "                else:\n",
        "                    tqdm.write(\"  Pan Balance Output: Left = 50.00%, Right = 50.00%\")\n",
        "            except Exception as e:\n",
        "                tqdm.write(f\"  [Error] Could not compute pan balance: {e}\")\n",
        "\n",
        "            if not normalize_audio and export_item['original_metrics'] is not None:\n",
        "                metrics = export_item['original_metrics']\n",
        "                if metrics['lufs'] is not None:\n",
        "                    tqdm.write(f\"  Original LUFS: {metrics['lufs']:.2f} LUFS\")\n",
        "                tqdm.write(f\"  Original Peak: {metrics['peak']:.2f} dBFS\")\n",
        "\n",
        "            norm_report = export_item['norm_report']\n",
        "            if norm_report is not None:\n",
        "                if norm_report['original_lufs'] is not None:\n",
        "                    tqdm.write(f\"  Original LUFS: {norm_report['original_lufs']:.2f} LUFS\")\n",
        "                else:\n",
        "                    tqdm.write(\"  Original LUFS: Not measured\")\n",
        "                if norm_report.get('original_peak') is not None:\n",
        "                    tqdm.write(f\"  Original PEAK: {norm_report['original_peak']:.2f} dBFS\")\n",
        "                else:\n",
        "                    tqdm.write(\"  Original PEAK: N/A\")\n",
        "                if norm_report['normalized_lufs'] is not None:\n",
        "                    tqdm.write(f\"  Normalized LUFS: {norm_report['normalized_lufs']:.2f} LUFS\")\n",
        "                else:\n",
        "                    tqdm.write(\"  Normalized LUFS: Not measured\")\n",
        "                tqdm.write(f\"  Normalized Peak: {norm_report['normalized_peak']:.2f} dBFS\")\n",
        "            elif normalize_audio:\n",
        "                try:\n",
        "                    y_exp2, _ = librosa.load(export_item['path'], sr=None, mono=False)\n",
        "                    peak_amp = np.max(np.abs(y_exp2))\n",
        "                    peak_db = 20 * np.log10(peak_amp) if peak_amp > 0 else -np.inf\n",
        "                    tqdm.write(f\"  Measured Peak: {peak_db:.2f} dBFS\")\n",
        "                except:\n",
        "                    tqdm.write(\"  [Error] Cannot measure peak\")\n",
        "\n",
        "            total_sil = file_results.get('total_silence', 0.0)\n",
        "            tqdm.write(f\"  Total silence removed: {format_duration(total_sil)}\")\n",
        "\n",
        "            device_str = \"GPU\" if cfg.use_cuda else \"CPU\"\n",
        "            tqdm.write(f\"  Processing Device: {device_str}\")\n",
        "\n",
        "    if failed_files:\n",
        "        tqdm.write(\"[Error] Some files failed:\")\n",
        "        for fn in failed_files:\n",
        "            tqdm.write(f\"  - {fn}\")\n",
        "\n",
        "    if not any_segmentation_failed:\n",
        "        tqdm.write(f\"[Info] Processing complete! Processed {len(files)-len(failed_files)} file(s) successfully.\")\n",
        "\n",
        "    # Zip output if requested\n",
        "    if zip_output == \"Yes\":\n",
        "        zip_path = output_path + '.zip'\n",
        "        shutil.make_archive(output_path, 'zip', output_path)\n",
        "        tqdm.write(f\"[Info] Output zipped to: {zip_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === RUN TESTS + MAIN, then DISPLAY AUDIO PLAYERS ===\n",
        "run_tests()\n",
        "results = main()\n",
        "\n",
        "print(\"\\n\\n--- Preprocessed Audio Previews ---\\n\")\n",
        "for file, file_results in results.items():\n",
        "    for export_item in file_results['export_paths']:\n",
        "        print(\"▶\", export_item['path'])\n",
        "        display(ipd.Audio(export_item['path']))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YbkV0PDyoJr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}