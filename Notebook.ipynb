{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5AcjXE/GHpKvqLVwNrtqC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"F8TtMNs80KTM"},"outputs":[],"source":["\n","# @title Mount Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"G1UcUJdQOLzG"},"outputs":[],"source":["# @title Install requirements\n","!apt-get update -qq && apt-get install -y ffmpeg\n","!pip install -q pydub numpy soundfile\n","!pip install resampy\n","!pip install gradio\n","!pip install numpy librosa matplotlib soundfile pydub ipython pyloudnorm ffmpeg-normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bW6Ldn-BZiQb"},"outputs":[],"source":["# @title Audio Preprocessor\n","import sys\n","import shutil\n","import os\n","import numpy as np\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import soundfile as sf\n","import IPython.display as ipd\n","from pydub import AudioSegment\n","import concurrent.futures\n","from tqdm import tqdm\n","import torch\n","import tempfile\n","import warnings\n","import pyloudnorm as pyln\n","import subprocess  # Added missing import\n","from dataclasses import dataclass\n","\n","# === 0. Dependency Check ===\n","def check_dependencies():\n","    \"\"\"Ensure all external binaries and Python packages are available.\"\"\"\n","    missing = []\n","    if shutil.which(\"ffmpeg\") is None:\n","        missing.append(\"ffmpeg (binary on PATH)\")\n","    if missing:\n","        sys.stderr.write(\"Missing dependencies:\\n\")\n","        for m in missing:\n","            sys.stderr.write(f\"  • {m}\\n\")\n","        sys.exit(1)\n","\n","check_dependencies()\n","\n","# Suppress librosa warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","# === Check GPU Availability ===\n","use_cuda_global = torch.cuda.is_available()\n","device_global = torch.device('cuda' if use_cuda_global else 'cpu')\n","print(f\"[Info] Using {'CUDA GPU' if use_cuda_global else 'CPU'} for processing\")\n","print(f\"[Info] Device: {device_global}\")\n","cpu_count = os.cpu_count() or 1\n","print(f\"[Info] Detected {cpu_count} CPU cores → Using \"\n","      f\"{max(1, min(cpu_count // 2, 4)) if use_cuda_global else max(1, min(cpu_count, 8))} workers\")\n","print()  # Blank line\n","\n","# === Audio Processing Configuration ===\n","# @markdown ---\n","# @markdown ## Input Settings\n","input_path = \"/content/drive/MyDrive/Drum Kits/Metro Grams Vol.1/snare/aggressive/snare_4.wav\"  # @param {type:\"string\"}\n","output_path = \"/content/processed/dataset\"  # @param {type:\"string\"}\n","\n","# @markdown ---\n","# @markdown ## Audio Format Settings\n","out_format = \"wav\"  # @param [\"wav\", \"mp3\", \"flac\", \"aiff\"]\n","sample_rate = \"41000\"  # @param [\"16000\", \"41000\", \"48000\", \"96000\"]\n","bit_depth = \"24\"  # @param [\"16\", \"24\", \"32\"]\n","channels = \"mono\"  # @param [\"mono\", \"stereo\"]\n","mp3_bitrate = \"192k\"  # @param [\"128k\", \"192k\", \"320k\"]\n","normalization_profile = \"Spotify\"  # @param [\"Spotify\", \"default\"]\n","# Define platform-specific loudness targets\n","profile_settings = {\n","    \"Spotify\": {\"lufs\": -14.0, \"peak\": -1.0},\n","    \"default\": {\"lufs\": None, \"peak\": None}  # No change\n","}\n","\n","# Set target values based on profile\n","if normalization_profile == \"default\":\n","    print(\"[Info] Default selected: using original audio LUFS and Peak.\")\n","    TARGET_LUFS = None\n","    TARGET_PEAK_DB = None\n","    normalize_audio = False  # Disable normalization for default\n","else:\n","    TARGET_LUFS = profile_settings[normalization_profile][\"lufs\"]\n","    TARGET_PEAK_DB = profile_settings[normalization_profile][\"peak\"]\n","    normalize_audio = True   # Enable normalization for Spotify\n","    print(f\"[Normalize] Target: {TARGET_LUFS} LUFS, {TARGET_PEAK_DB} dBTP for {normalization_profile}\")\n","\n","# User setting: panning correction enabled or not (Yes/No)\n","panning_correction = \"Yes\"  # @param [\"Yes\", \"No\"]\n","# Convert to boolean\n","panning_correction = (panning_correction == \"Yes\")\n","if panning_correction:\n","    print(\"[Info] Panning correction is enabled.\")\n","else:\n","    print(\"[Info] Panning correction is disabled.\")\n","\n","visualize = True  # @param {type:\"boolean\"}\n","\n","# @markdown ---\n","# @markdown ## Segmentation Options\n","segmentation = False  # @param {type:\"boolean\"}\n","duration = 30  # @param {type:\"slider\", min:1, max:60, step:1}\n","time_unit = \"Seconds\"  # @param [\"Milliseconds\", \"Seconds\", \"Minutes\", \"Hours\"]\n","\n","# === Auto Worker Configuration ===\n","num_workers = max(1, min(cpu_count // 2, 4)) if use_cuda_global else max(1, min(cpu_count, 8))\n","\n","# === Directory Handling ===\n","format_subfolder = {'wav': 'wav', 'mp3': 'mp3', 'flac': 'flac', 'aiff': 'aiff'}[out_format]\n","export = os.path.join(output_path, format_subfolder)\n","os.makedirs(export, exist_ok=True)\n","\n","# === Constants ===\n","TOP_DB = 40\n","DB_THRESH = -45\n","EDGE_SILENCE_THRESHOLD = 3e-3\n","VALID_FORMATS = ('.wav', '.mp3', '.flac', '.aiff')\n","MIN_FRAMES_FOR_RMS = 50\n","DEFAULT_HOP_LENGTH = 512\n","\n","# === Loudness Normalization Helpers ===\n","MIN_BLOCK_SIZE = 0.050  # 50 ms\n","MIN_BLOCK_FLOOR = 0.001  # 1 ms\n","\n","# === 1. Centralized Config ===\n","@dataclass(frozen=True)\n","class Config:\n","    sample_rate: int\n","    bit_depth: str\n","    channels: str\n","    normalization_profile: str\n","    TARGET_LUFS: float\n","    TARGET_PEAK_DB: float\n","    use_cuda: bool\n","    device: torch.device\n","    visualize: bool\n","    segmentation: bool\n","    duration_seconds: float\n","    panning_correction: bool\n","    mp3_bitrate: str\n","\n","cfg = Config(\n","    sample_rate=int(sample_rate),\n","    bit_depth=bit_depth,\n","    channels=channels,\n","    normalization_profile=normalization_profile,\n","    TARGET_LUFS=TARGET_LUFS,\n","    TARGET_PEAK_DB=TARGET_PEAK_DB,\n","    use_cuda=use_cuda_global,\n","    device=device_global,\n","    visualize=visualize,\n","    segmentation=segmentation,\n","    duration_seconds=(\n","        duration if time_unit == \"Seconds\"\n","        else duration / 1000.0 if time_unit == \"Milliseconds\"\n","        else duration * 60 if time_unit == \"Minutes\"\n","        else duration * 3600\n","    ),\n","    panning_correction=panning_correction,\n","    mp3_bitrate=mp3_bitrate\n",")\n","\n","def measure_loudness(y: np.ndarray, sr: int) -> dict:\n","    \"\"\"\n","    Measure integrated LUFS and peak dB of an audio array.\n","\n","    Parameters\n","    ----------\n","    y : np.ndarray\n","        Audio samples (mono or multichannel).\n","    sr : int\n","        Sample rate in Hz.\n","\n","    Returns\n","    -------\n","    dict\n","        { 'lufs': float | None, 'peak': float }\n","    \"\"\"\n","    BLOCK_SECONDS = 0.400\n","    y_mono = np.mean(y, axis=0) if y.ndim > 1 else y\n","    min_len = int(BLOCK_SECONDS * sr)\n","    if len(y_mono) < min_len:\n","        y_mono_padded = np.pad(y_mono, (0, min_len - len(y_mono)), mode=\"constant\")\n","    else:\n","        y_mono_padded = y_mono\n","    meter = pyln.Meter(sr, block_size=BLOCK_SECONDS)\n","    try:\n","        lufs = meter.integrated_loudness(y_mono_padded)\n","    except:\n","        lufs = None\n","    peak_amp = np.max(np.abs(y))\n","    peak_db = 20 * np.log10(peak_amp) if peak_amp > 0 else -np.inf\n","    return {'lufs': lufs, 'peak': peak_db}\n","\n","def normalize_loudness_true(y: np.ndarray, sr: int, log_message_func) -> tuple:\n","    \"\"\"\n","    True-LUFS normalization with peak limiting fallback.\n","\n","    Parameters\n","    ----------\n","    y : np.ndarray\n","    sr : int\n","    log_message_func : callable\n","\n","    Returns\n","    -------\n","    tuple\n","        y_norm : np.ndarray\n","        report : dict\n","    \"\"\"\n","    BLOCK_SECONDS = 0.400\n","    orig_peak_amp = np.max(np.abs(y))\n","    orig_peak_db = 20 * np.log10(orig_peak_amp) if orig_peak_amp > 0 else -np.inf\n","    log_message_func(f\"[Normalize] Original Peak: {orig_peak_db:.2f} dBFS\")\n","    y_mono = np.mean(y, axis=0) if y.ndim > 1 else y\n","    min_len = int(BLOCK_SECONDS * sr)\n","    if len(y_mono) < min_len:\n","        y_mono_padded = np.pad(y_mono, (0, min_len - len(y_mono)), mode=\"constant\")\n","    else:\n","        y_mono_padded = y_mono\n","    meter = pyln.Meter(sr, block_size=BLOCK_SECONDS)\n","    try:\n","        orig_lufs = meter.integrated_loudness(y_mono_padded)\n","        log_message_func(f\"[Normalize] Original LUFS: {orig_lufs:.2f} LUFS\")\n","    except Exception as e:\n","        log_message_func(f\"[Normalize] LUFS measurement failed: {e}\")\n","        return normalize_by_peak_only(y, sr, log_message_func)\n","    # Use global TARGET_LUFS instead of cfg.TARGET_LUFS\n","    gain_lin = 10 ** ((TARGET_LUFS - orig_lufs) / 20)\n","    y_lufs = y * gain_lin\n","    log_message_func(f\"[Normalize] Applied gain: {TARGET_LUFS - orig_lufs:.2f} dB to reach target LUFS\")\n","    peak_amp_after = np.max(np.abs(y_lufs))\n","    peak_db_after = 20 * np.log10(peak_amp_after) if peak_amp_after > 0 else -np.inf\n","    log_message_func(f\"[Normalize] Peak after LUFS: {peak_db_after:.2f} dBFS\")\n","    # Use global TARGET_PEAK_DB instead of cfg.TARGET_PEAK_DB\n","    if peak_db_after > TARGET_PEAK_DB:\n","        scale = (10 ** (TARGET_PEAK_DB / 20)) / peak_amp_after\n","        y_norm = y_lufs * scale\n","        log_message_func(f\"[Normalize] Applied peak limit to {TARGET_PEAK_DB:.1f} dBFS\")\n","    else:\n","        y_norm = y_lufs\n","        log_message_func(\"[Normalize] No peak limiting needed\")\n","    y_norm_mono = np.mean(y_norm, axis=0) if y_norm.ndim > 1 else y_norm\n","    if len(y_norm_mono) < min_len:\n","        y_norm_mono_padded = np.pad(y_norm_mono, (0, min_len - len(y_norm_mono)), mode=\"constant\")\n","    else:\n","        y_norm_mono_padded = y_norm_mono\n","    try:\n","        final_lufs = meter.integrated_loudness(y_norm_mono_padded)\n","        log_message_func(f\"[Normalize] Final LUFS: {final_lufs:.2f} LUFS\")\n","    except:\n","        final_lufs = None\n","    final_peak_amp = np.max(np.abs(y_norm))\n","    final_peak_db = 20 * np.log10(final_peak_amp) if final_peak_amp > 0 else -np.inf\n","    log_message_func(f\"[Normalize] Final Peak: {final_peak_db:.2f} dBFS\")\n","    return y_norm, {\n","        'original_lufs':   orig_lufs,\n","        'original_peak':   orig_peak_db,\n","        'normalized_lufs': final_lufs,\n","        'normalized_peak': final_peak_db,\n","        'method':          'true_lufs',\n","        'duration':        len(y_norm) / sr\n","    }\n","\n","def normalize_by_peak_only(y: np.ndarray, sr: int, log_message_func) -> tuple:\n","    \"\"\"\n","    Peak-only normalization fallback.\n","\n","    Parameters\n","    ----------\n","    y : np.ndarray\n","    sr : int\n","    log_message_func : callable\n","\n","    Returns\n","    -------\n","    tuple\n","        y_norm : np.ndarray\n","        report : dict\n","    \"\"\"\n","    log_message_func(\"[Normalize] Applying peak-only normalization\")\n","    orig_peak = np.max(np.abs(y))\n","    orig_peak_db = 20 * np.log10(orig_peak) if orig_peak > 0 else -np.inf\n","    log_message_func(f\"[Normalize] Original Peak: {orig_peak_db:.2f} dBFS\")\n","    # Use global TARGET_PEAK_DB instead of cfg.TARGET_PEAK_DB\n","    if orig_peak > 0 and TARGET_PEAK_DB is not None:\n","        scale = (10 ** (TARGET_PEAK_DB / 20)) / orig_peak\n","        y_norm = y * scale\n","        final_peak_db = TARGET_PEAK_DB\n","    else:\n","        y_norm = y\n","        final_peak_db = orig_peak_db\n","    log_message_func(f\"[Normalize] Final Peak (peak-only): {final_peak_db:.2f} dBFS\")\n","    return y_norm, {\n","        'original_lufs':   None,\n","        'original_peak':   orig_peak_db,\n","        'normalized_lufs': None,\n","        'normalized_peak': final_peak_db,\n","        'method':          'peak_only',\n","        'duration':        len(y_norm) / sr\n","    }\n","\n","def normalize_by_peak_only(y: np.ndarray, sr: int, log_message_func) -> tuple:\n","    \"\"\"\n","    Peak-only normalization fallback.\n","\n","    Parameters\n","    ----------\n","    y : np.ndarray\n","    sr : int\n","    log_message_func : callable\n","\n","    Returns\n","    -------\n","    tuple\n","        y_norm : np.ndarray\n","        report : dict\n","    \"\"\"\n","    log_message_func(\"[Normalize] Applying peak-only normalization\")\n","    orig_peak = np.max(np.abs(y))\n","    orig_peak_db = 20 * np.log10(orig_peak) if orig_peak > 0 else -np.inf\n","    log_message_func(f\"[Normalize] Original Peak: {orig_peak_db:.2f} dBFS\")\n","    if orig_peak > 0 and cfg.TARGET_PEAK_DB is not None:\n","        scale = (10 ** (cfg.TARGET_PEAK_DB / 20)) / orig_peak\n","        y_norm = y * scale\n","        final_peak_db = cfg.TARGET_PEAK_DB\n","    else:\n","        y_norm = y\n","        final_peak_db = orig_peak_db\n","    log_message_func(f\"[Normalize] Final Peak (peak-only): {final_peak_db:.2f} dBFS\")\n","    return y_norm, {\n","        'original_lufs':   None,\n","        'original_peak':   orig_peak_db,\n","        'normalized_lufs': None,\n","        'normalized_peak': final_peak_db,\n","        'method':          'peak_only',\n","        'duration':        len(y_norm) / sr\n","    }\n","\n","def pan_percent(left: np.ndarray, right: np.ndarray) -> tuple:\n","    \"\"\"\n","    Compute percent power in left/right channels and deviation from center.\n","    \"\"\"\n","    power_left = np.sum(left ** 2)\n","    power_right = np.sum(right ** 2)\n","    total = power_left + power_right\n","    if total < 1e-10:\n","        return 50.0, 50.0, 0.0\n","    left_pct = (power_left / total) * 100\n","    right_pct = (power_right / total) * 100\n","    return left_pct, right_pct, abs(left_pct - 50)\n","\n","def normalize_panning(audio: np.ndarray, log_message_func) -> np.ndarray:\n","    \"\"\"\n","    Apply corrective gain to right channel to match left RMS.\n","\n","    Parameters\n","    ----------\n","    audio : np.ndarray\n","    log_message_func : callable\n","\n","    Returns\n","    -------\n","    np.ndarray\n","    \"\"\"\n","    if audio.ndim == 1:\n","        log_message_func(\"[Panning] Converting mono to stereo for correction\")\n","        audio = np.stack([audio, audio], axis=0)\n","    left, right = audio[0], audio[1]\n","    rms_left = np.sqrt(np.mean(left ** 2))\n","    rms_right = np.sqrt(np.mean(right ** 2))\n","    if rms_right < 1e-6:\n","        return audio\n","    log_message_func(\"[Panning] Applying panning correction\")\n","    return np.vstack((left, right * (rms_left / rms_right)))\n","\n","def dbfs(amplitude: float) -> float:\n","    \"\"\"\n","    Convert linear amplitude to dBFS.\n","    \"\"\"\n","    return 20 * np.log10(amplitude) if amplitude > 0 else -np.inf\n","\n","def soft_limiter(signal: np.ndarray, threshold_db: float=-3.00, ratio: float=10) -> np.ndarray:\n","    \"\"\"\n","    Soft clip any samples above threshold with given ratio.\n","\n","    Parameters\n","    ----------\n","    signal : np.ndarray\n","    threshold_db : float\n","    ratio : float\n","\n","    Returns\n","    -------\n","    np.ndarray\n","    \"\"\"\n","    threshold_lin = 10 ** (threshold_db / 20)\n","    if signal.ndim > 1:\n","        limited = np.zeros_like(signal)\n","        for c in range(signal.shape[0]):\n","            ch = signal[c]\n","            abs_ch = np.abs(ch)\n","            above = abs_ch > threshold_lin\n","            limited[c, ~above] = ch[~above]\n","            limited[c, above] = np.sign(ch[above]) * threshold_lin * ((abs_ch[above] / threshold_lin) ** (1/ratio))\n","        return limited\n","    else:\n","        abs_sig = np.abs(signal)\n","        limited = signal.copy()\n","        above = abs_sig > threshold_lin\n","        limited[above] = np.sign(signal[above]) * threshold_lin * ((abs_sig[above] / threshold_lin) ** (1/ratio))\n","        return limited\n","\n","def format_duration(seconds: float) -> str:\n","    \"\"\"\n","    Format a duration into the current time_unit.\n","    \"\"\"\n","    if time_unit == \"Milliseconds\":\n","        ms = seconds * 1000\n","        return f\"{ms:.2f} ms\" if ms < 1000 else f\"{seconds:.4f} s\"\n","    if time_unit == \"Seconds\":\n","        return f\"{seconds:.4f} s\"\n","    if time_unit == \"Minutes\":\n","        return f\"{seconds/60:.4f} min\"\n","    if time_unit == \"Hours\":\n","        return f\"{seconds/3600:.4f} hours\"\n","    return f\"{seconds:.4f} s\"\n","\n","def detect_clipping(y: np.ndarray, use_cuda: bool, device) -> tuple:\n","    \"\"\"\n","    Detect sample clipping beyond ±0.999.\n","\n","    Returns\n","    -------\n","    (clipped: bool, ratio: float)\n","    \"\"\"\n","    if use_cuda:\n","        try:\n","            t = torch.tensor(y, device=device)\n","            mask = torch.abs(t) >= 0.999\n","            ratio = mask.float().mean().item()\n","            return ratio > 0.001, ratio\n","        except RuntimeError:\n","            mask = np.abs(y) >= 0.999\n","            ratio = mask.sum() / y.size\n","            return ratio > 0.001, ratio\n","    else:\n","        mask = np.abs(y) >= 0.999\n","        ratio = mask.sum() / y.size\n","        return ratio > 0.001, ratio\n","\n","def attenuate_audio(y: np.ndarray, use_cuda: bool, device) -> np.ndarray:\n","    \"\"\"\n","    Scale down waveform to avoid clipping, leaving 0.05 headroom.\n","    \"\"\"\n","    if use_cuda:\n","        try:\n","            t = torch.tensor(y, device=device)\n","            peak = torch.max(torch.abs(t)).item()\n","            if peak == 0: return y\n","            factor = min(1.0 - 0.05, 1.0 / peak)\n","            return (t * factor).cpu().numpy()\n","        except RuntimeError:\n","            peak = np.max(np.abs(y))\n","            if peak == 0: return y\n","            factor = min(1.0 - 0.05, 1.0 / peak)\n","            return y * factor\n","    else:\n","        peak = np.max(np.abs(y))\n","        if peak == 0: return y\n","        factor = min(1.0 - 0.05, 1.0 / peak)\n","        return y * factor\n","\n","def calculate_adaptive_hop_length(length: int) -> int:\n","    \"\"\"\n","    Determine hop length for RMS frames based on total length.\n","    \"\"\"\n","    return min(DEFAULT_HOP_LENGTH, max(32, length // MIN_FRAMES_FOR_RMS))\n","\n","def auto_slice_audio(y: np.ndarray, sr: int, use_cuda: bool, device, log_message_func) -> tuple:\n","    \"\"\"\n","    Rough silence trimming via frame-based RMS threshold.\n","    \"\"\"\n","    L = len(y)\n","    if L < 128: return y, 0, L\n","    hop = calculate_adaptive_hop_length(L)\n","    frame_length = min(2048, L)\n","    if use_cuda:\n","        try:\n","            t = torch.tensor(y, device=device)\n","            frames = t.unfold(0, frame_length, hop)\n","            rms = torch.sqrt((frames ** 2).mean(dim=1))\n","            db = 20 * torch.log10(rms / rms.max() + 1e-7)\n","            db = db.cpu().numpy()\n","        except RuntimeError:\n","            log_message_func(\"[Silence] GPU OOM; falling back to CPU RMS\")\n","            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop)[0]\n","            db = librosa.amplitude_to_db(rms, ref=np.max)\n","    else:\n","        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop)[0]\n","        db = librosa.amplitude_to_db(rms, ref=np.max)\n","    idx = np.where(db > DB_THRESH)[0]\n","    if idx.size == 0: return y, 0, L\n","    start = idx[0] * hop\n","    end = min(L, (idx[-1] + 1) * hop)\n","    return y[start:end], start, end\n","\n","def slice_edge_silence(y: np.ndarray, thresh: float=EDGE_SILENCE_THRESHOLD) -> tuple:\n","    \"\"\"\n","    Precise trim of very low-level edge silence.\n","    \"\"\"\n","    abs_y = np.abs(y)\n","    nz = np.where(abs_y > thresh)[0]\n","    if nz.size == 0: return y, 0, len(y)\n","    return y[nz[0]: nz[-1] + 1], nz[0], nz[-1] + 1\n","\n","def hard_slice_to_zero(y: np.ndarray, thresh: float=EDGE_SILENCE_THRESHOLD) -> np.ndarray:\n","    \"\"\"\n","    Zero out samples before/after threshold crossings.\n","    \"\"\"\n","    abs_y = np.abs(y)\n","    if not (abs_y > thresh).any(): return y\n","    s = np.argmax(abs_y > thresh)\n","    e = len(y) - np.argmax(abs_y[::-1] > thresh)\n","    y[:s] = 0; y[e:] = 0\n","    return y\n","\n","def process_silence(y: np.ndarray, sr: int, use_cuda: bool, device, log_message_func) -> tuple:\n","    \"\"\"\n","    Full two-stage silence trimming pipeline.\n","    \"\"\"\n","    y_mono = librosa.to_mono(y) if y.ndim > 1 else y\n","    tmono, s0, e0 = auto_slice_audio(y_mono, sr, use_cuda, device, log_message_func)\n","    trimmed = (y[:, s0:e0] if y.ndim > 1 else y[s0:e0])\n","    fine_m, fs, fe = slice_edge_silence(tmono)\n","    fine = (trimmed[:, fs:fe] if y.ndim > 1 else trimmed[fs:fe])\n","    if fine.ndim > 1:\n","        proc = fine.copy()\n","        for c in range(proc.shape[0]):\n","            proc[c] = hard_slice_to_zero(fine[c], thresh=EDGE_SILENCE_THRESHOLD)\n","    else:\n","        proc = hard_slice_to_zero(fine.copy(), thresh=EDGE_SILENCE_THRESHOLD)\n","    proc[np.abs(proc) < 1e-6] = 0\n","    start = s0 + fs\n","    end = start + (fe - fs)\n","    pre = start / sr\n","    post = (len(y_mono) - end) / sr\n","    total = pre + post\n","    return proc, (pre, post, total), (start, end)\n","\n","def plot_trim_boundaries(y: np.ndarray, sr: int, s0: int, e0: int, segments=None):\n","    \"\"\"\n","    Plot waveform with trim boundaries (and optional segment markers).\n","    \"\"\"\n","    if not cfg.visualize: return\n","    unit_dict = {\"Milliseconds\":(\"ms\",1000),\"Seconds\":(\"s\",1),\n","                 \"Minutes\":(\"min\",1/60),\"Hours\":(\"hours\",1/3600)}\n","    unit, fac = unit_dict.get(time_unit, (\"s\",1))\n","    times = np.arange(y.shape[-1]) / sr * fac\n","    plt.figure(figsize=(14,3))\n","    plt.plot(times, y.T if y.ndim>1 else y, alpha=0.8)\n","    plt.axvline(s0/sr*fac, linestyle='--', color='red', label='Trim Boundary')\n","    plt.axvline(e0/sr*fac, linestyle='--', color='red')\n","    if segments:\n","        for ss, ee in segments:\n","            plt.axvline(ss/sr*fac, linestyle='-', color='green', alpha=0.7)\n","            plt.axvline(ee/sr*fac, linestyle='-', color='green', alpha=0.7)\n","        plt.title(f\"Waveform with Trim Boundaries (Red) and Segments (Green) ({time_unit})\")\n","    else:\n","        plt.title(f\"Waveform with Trim Boundaries (Red) ({time_unit})\")\n","    plt.xlabel(f\"Time ({unit})\")\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_zoomed_silence(y: np.ndarray, sr: int, s0: int, e0: int, zoom: float=0.05):\n","    \"\"\"\n","    Plot a zoomed-in view of the silence before/after trim.\n","    \"\"\"\n","    if not cfg.visualize: return\n","    zs = int(sr * zoom)\n","    unit_dict = {\"Milliseconds\":(\"ms\",1000),\"Seconds\":(\"s\",1),\n","                 \"Minutes\":(\"min\",1/60),\"Hours\":(\"hours\",1/3600)}\n","    unit, fac = unit_dict.get(time_unit, (\"s\",1))\n","    length = y.shape[-1] if y.ndim>1 else y.size\n","    if s0 > zs:\n","        pre = y[:,s0-zs:s0] if y.ndim>1 else y[s0-zs:s0]\n","        t0 = np.linspace(-zoom*fac,0,pre.shape[-1])\n","    else:\n","        pre = y[:s0] if y.ndim>1 else y[:s0]\n","        t0 = np.linspace(-pre.shape[-1]/sr*fac,0,pre.shape[-1])\n","    if e0+zs < length:\n","        post = y[:,e0:e0+zs] if y.ndim>1 else y[e0:e0+zs]\n","        t1 = np.linspace(0,zoom*fac,post.shape[-1])\n","    else:\n","        post = y[:,e0:] if y.ndim>1 else y[e0:]\n","        t1 = np.linspace(0,post.shape[-1]/sr*fac,post.shape[-1])\n","    fig, axs = plt.subplots(2,1,figsize=(14,4))\n","    if pre.shape[-1] > 0:\n","        axs[0].plot(t0, pre.T if pre.ndim>1 else pre)\n","        axs[0].set_xlim(t0[0], t0[-1])\n","    else:\n","        axs[0].text(0.5,0.5,\"No silence before trim\",ha='center',va='center')\n","        axs[0].set_xlim(-1,1)\n","    axs[0].set_title(f\"Zoomed Silence Before Trim ({time_unit})\")\n","    axs[0].set_xlabel(f\"Time ({unit})\")\n","    if post.shape[-1] > 0:\n","        axs[1].plot(t1, post.T if post.ndim>1 else post)\n","        axs[1].set_xlim(t1[0], t1[-1])\n","    else:\n","        axs[1].text(0.5,0.5,\"No silence after trim\",ha='center',va='center')\n","        axs[1].set_xlim(-1,1)\n","    axs[1].set_title(f\"Zoomed Silence After Trim ({time_unit})\")\n","    axs[1].set_xlabel(f\"Time ({unit})\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_normalized_waveform(y: np.ndarray, sr: int, title: str=\"Normalized Waveform\"):\n","    \"\"\"\n","    Plot the final processed or normalized waveform.\n","    \"\"\"\n","    if not cfg.visualize: return\n","    length = y.shape[-1] if y.ndim>1 else y.size\n","    if length == 0: return\n","    unit_dict = {\"Milliseconds\":(\"ms\",1000),\"Seconds\":(\"s\",1),\n","                 \"Minutes\":(\"min\",1/60),\"Hours\":(\"hours\",1/3600)}\n","    unit, fac = unit_dict.get(time_unit, (\"s\",1))\n","    times = np.arange(length) / sr * fac\n","    plt.figure(figsize=(14,3))\n","    if y.ndim>1:\n","        for c in range(y.shape[0]):\n","            plt.plot(times, y[c], alpha=0.7, label=f'Channel {c+1}')\n","        plt.legend()\n","    else:\n","        plt.plot(times, y, alpha=0.8)\n","    plt.title(f\"{title} ({time_unit})\")\n","    plt.xlabel(f\"Time ({unit})\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","def get_all_audio_files(path: str) -> list:\n","    \"\"\"\n","    Recursively collect all valid audio files under path.\n","    \"\"\"\n","    files = []\n","    if os.path.isfile(path) and path.lower().endswith(VALID_FORMATS):\n","        files.append(path)\n","    else:\n","        for root, _, fnames in os.walk(path):\n","            for f in fnames:\n","                if f.lower().endswith(VALID_FORMATS):\n","                    files.append(os.path.join(root, f))\n","    return files\n","\n","def get_export_path(file_path: str, input_dir: str, is_segmented: bool=False) -> str:\n","    \"\"\"\n","    Compute and create export directory for a given file.\n","    \"\"\"\n","    if os.path.isdir(input_dir):\n","        rel_path = os.path.relpath(file_path, input_dir)\n","        parts = rel_path.split(os.sep)\n","        volume_name = parts[0] if len(parts) > 1 else os.path.basename(input_dir)\n","    else:\n","        volume_name = os.path.splitext(os.path.basename(input_dir))[0]\n","    volume_name = volume_name.replace(' ', '_')\n","    base_path = os.path.join(export, volume_name)\n","    if is_segmented:\n","        file_name = os.path.splitext(os.path.basename(file_path))[0]\n","        base_path = os.path.join(base_path, f\"{file_name}_segments\")\n","    os.makedirs(base_path, exist_ok=True)\n","    return base_path\n","\n","def export_audio(y: np.ndarray, sr: int, orig: str, fmt: str, idx: int=None) -> str:\n","    \"\"\"\n","    Write processed audio to disk in the desired format.\n","    \"\"\"\n","    is_segmented = idx is not None\n","    export_path = get_export_path(orig, input_path, is_segmented)\n","    base = os.path.splitext(os.path.basename(orig))[0]\n","    if cfg.channels == \"mono\" and y.ndim > 1:\n","        y = y.mean(axis=0)\n","    elif cfg.channels == \"stereo\" and y.ndim == 1:\n","        y = np.stack([y, y], axis=0)\n","    name = f\"{base}_segment_{idx+1}.{fmt}\" if is_segmented else f\"{base}.{fmt}\"\n","    out_path = os.path.join(export_path, name)\n","    if fmt.lower() == \"mp3\":\n","        tmp = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n","        tmp_path = tmp.name\n","        tmp.close()\n","        sf.write(tmp_path, y.T if y.ndim>1 else y, sr, subtype='FLOAT')\n","        try:\n","            cmd = [\n","                \"ffmpeg\", \"-y\", \"-i\", tmp_path,\n","                \"-c:a\", \"libmp3lame\",\n","                \"-b:a\", cfg.mp3_bitrate,\n","                \"-ar\", str(sr),\n","                out_path\n","            ]\n","            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n","        finally:\n","            os.remove(tmp_path)\n","    else:\n","        actual_bit_depth = cfg.bit_depth\n","        if fmt == \"flac\" and cfg.bit_depth == \"32\":\n","            print(\"[Warning] 32-bit FLAC not supported; switching to 24-bit.\")\n","            actual_bit_depth = \"24\"\n","        subtype = f\"PCM_{actual_bit_depth}\"\n","        sf.write(out_path, y.T if y.ndim>1 else y, sr, subtype=subtype)\n","    return out_path\n","\n","def process_file(file: str, params: dict) -> tuple:\n","    \"\"\"\n","    End-to-end processing of a single audio file, including trimming,\n","    normalization, segmentation, panning, and export.\n","\n","    Returns\n","    -------\n","    tuple\n","        (file_path, results_dict)\n","    \"\"\"\n","    file_results = {\n","        'messages': [],\n","        'export_paths': [],\n","        'plot_data': {},\n","        'total_silence': 0.0,\n","        'norm_reports': [],\n","        'original_metrics': None\n","    }\n","    def log_message(msg):\n","        file_results['messages'].append(msg)\n","    try:\n","        use_cuda = params['use_cuda']\n","        device = params['device']\n","        sample_rate_val = int(params['sample_rate'])\n","        current_segmentation = params['segmentation']\n","        duration_seconds = params['duration_seconds']\n","        time_unit_local = params['time_unit']\n","        normalize_audio_flag = params['normalize_audio']\n","        panning_correction_flag = params['panning_correction']\n","\n","        log_message(f\"[Info] Processing File: {file}\")\n","        log_message(f\"[Info] Processing Device: {'GPU' if use_cuda else 'CPU'}\")\n","\n","        try:\n","            y, sr = librosa.load(file, sr=sample_rate_val, mono=False)\n","        except Exception as e:\n","            log_message(f\"[Error] Could not load audio: {e}\")\n","            y, sr = librosa.load(file, sr=sample_rate_val, mono=True)\n","\n","        log_message(\"\")  # Blank line\n","        try:\n","            info = sf.info(file)\n","            log_message(\"[Info] Input Audio Details:\")\n","            log_message(f\"  Path: {file}\")\n","            log_message(f\"  Format: {os.path.splitext(file)[1][1:].upper()}\")\n","            log_message(f\"  Sample Rate: {info.samplerate} Hz\")\n","            log_message(f\"  Bit Depth: {info.subtype}\")\n","            log_message(f\"  Channels: {info.channels}\")\n","            log_message(f\"  Duration: {format_duration(info.duration)}\")\n","            if y.ndim > 1 and y.shape[0] == 2:\n","                lp, rp, _ = pan_percent(y[0], y[1])\n","                log_message(f\"  Pan Balance Input: Left = {lp:.2f}%, Right = {rp:.2f}%\")\n","        except Exception as e:\n","            log_message(f\"[Error] Could not read metadata: {e}\")\n","\n","        original_channels = \"mono\" if y.ndim == 1 else \"stereo\"\n","        log_message(f\"[Info] Original audio channels: {original_channels}\")\n","\n","        if cfg.channels == \"mono\" and y.ndim > 1:\n","            log_message(\"[Info] Converting stereo → mono\")\n","            y = librosa.to_mono(y)\n","        elif cfg.channels == \"stereo\" and y.ndim == 1:\n","            log_message(\"[Info] Converting mono → stereo\")\n","            y = np.stack([y, y], axis=0)\n","\n","        if y.ndim > 1 and y.shape[0] == 2:\n","            lp, rp, _ = pan_percent(y[0], y[1])\n","            log_message(f\"[Info] Pan After Conversion: Left = {lp:.2f}%, Right = {rp:.2f}%\")\n","\n","        if panning_correction_flag:\n","            y = normalize_panning(y, log_message)\n","            if cfg.channels == \"mono\":\n","                y = librosa.to_mono(y)\n","                log_message(\"[Panning] Converted corrected stereo back to mono\")\n","            else:\n","                lp, rp, _ = pan_percent(y[0], y[1])\n","                log_message(f\"[Panning] Corrected Pan Balance: Left = {lp:.2f}%, Right = {rp:.2f}%\")\n","\n","        clipped, ratio = detect_clipping(y, use_cuda, device)\n","        if clipped:\n","            log_message(f\"[Clipping] Detected {ratio:.2%} clipped → attenuating\")\n","            y = attenuate_audio(y, use_cuda, device)\n","\n","        y_proc, (pre, post, total_silence), (s_start, s_end) = process_silence(\n","            y, sr, use_cuda, device, log_message\n","        )\n","        log_message(\"\")\n","        log_message(\"[Silence] Trim Results:\")\n","        log_message(f\"  Start silence: {format_duration(pre)}\")\n","        log_message(f\"  End silence: {format_duration(post)}\")\n","        log_message(f\"  Total silence removed: {format_duration(total_silence)}\")\n","        file_results['total_silence'] = total_silence\n","\n","        if not normalize_audio_flag:\n","            try:\n","                log_message(\"[Info] Measuring original loudness\")\n","                orig_metrics = measure_loudness(y_proc, sr)\n","                file_results['original_metrics'] = orig_metrics\n","                if orig_metrics['lufs'] is not None:\n","                    log_message(f\"  Original LUFS: {orig_metrics['lufs']:.2f} LUFS\")\n","                log_message(f\"  Original Peak: {orig_metrics['peak']:.2f} dBFS\")\n","            except Exception as e:\n","                log_message(f\"[Error] Loudness measure failed: {e}\")\n","\n","        norm_report = None\n","        if normalize_audio_flag and not current_segmentation:\n","            log_message(\"\")\n","            log_message(\"[Normalize] Starting Loudness Normalization (full)\")\n","            try:\n","                y_proc, norm_report = normalize_loudness_true(y_proc, sr, log_message)\n","                log_message(\"[Normalize] Completed full normalization\")\n","            except Exception as e:\n","                log_message(f\"[Normalize] Failed: {e}\")\n","        file_results['norm_reports'].append(norm_report)\n","\n","        processed_duration = y_proc.shape[-1] / sr\n","        export_paths = []\n","        segments = []\n","        seg_failed = False\n","\n","        if current_segmentation:\n","            log_message(f\"[Segmentation] Segment duration: {format_duration(duration_seconds)}\")\n","            log_message(f\"[Segmentation] Processed duration: {format_duration(processed_duration)}\")\n","\n","            if processed_duration < duration_seconds:\n","                log_message(\"[Error] Audio shorter than segment duration → skipping segmentation\")\n","                seg_failed = True\n","                current_segmentation = False\n","            else:\n","                seg_len = int(sr * duration_seconds)\n","                total_len = y_proc.shape[-1]\n","                n_full = total_len // seg_len\n","                for i in range(n_full):\n","                    segments.append((i*seg_len, (i+1)*seg_len))\n","                rem = total_len % seg_len\n","                if rem > sr:\n","                    segments.append((n_full*seg_len, total_len))\n","                    log_message(f\"[Segmentation] Final short segment: {format_duration(rem/sr)}\")\n","                elif rem > 0:\n","                    log_message(f\"[Segmentation] Skipping tiny remainder ({format_duration(rem/sr)})\")\n","                log_message(\"\")\n","                log_message(\"[Segmentation] Results:\")\n","                log_message(f\"  Total segments: {len(segments)}\")\n","                log_message(\"\")\n","                log_message(\"[Summary] Processing:\")\n","                log_message(f\"  Total silence removed: {format_duration(total_silence)}\")\n","                log_message(f\"  Total segments: {len(segments)}\")\n","        else:\n","            log_message(\"\")\n","            log_message(\"[Summary] Processing:\")\n","            log_message(f\"  Total silence removed: {format_duration(total_silence)}\")\n","            log_message(\"  Exporting single file\")\n","\n","        file_results['plot_data'] = {\n","            'y': y,\n","            'y_proc': y_proc,\n","            'sr': sr,\n","            'samp_start': s_start,\n","            'samp_end': s_end,\n","            'segments': segments if segments else None,\n","            'segmentation_failed': seg_failed,\n","            'current_segmentation': current_segmentation,\n","            'total_len': y_proc.shape[-1] if y_proc is not None else 0\n","        }\n","\n","        if current_segmentation and segments and not seg_failed:\n","            for i, (st, en) in enumerate(segments):\n","                seg_audio = y_proc[:, st:en] if y_proc.ndim > 1 else y_proc[st:en]\n","                seg_report = None\n","                if normalize_audio_flag:\n","                    log_message(\"\")\n","                    log_message(f\"[Normalize] Segment {i+1} normalization\")\n","                    try:\n","                        seg_audio, seg_report = normalize_loudness_true(seg_audio, sr, log_message)\n","                        log_message(f\"[Normalize] Completed segment {i+1}\")\n","                    except Exception as e:\n","                        log_message(f\"[Normalize] Segment {i+1} failed: {e}\")\n","                file_results['norm_reports'].append(seg_report)\n","                path = export_audio(seg_audio, sr, file, out_format, i)\n","                export_paths.append(path)\n","                file_results['export_paths'].append({\n","                    'path': path,\n","                    'seg_index': i,\n","                    'seg_duration': seg_audio.shape[-1]/sr,\n","                    'norm_report': seg_report,\n","                    'original_metrics': file_results['original_metrics']\n","                })\n","        elif not seg_failed:\n","            path = export_audio(y_proc, sr, file, out_format)\n","            export_paths.append(path)\n","            file_results['export_paths'].append({\n","                'path': path,\n","                'seg_index': None,\n","                'seg_duration': processed_duration,\n","                'norm_report': norm_report,\n","                'original_metrics': file_results['original_metrics']\n","            })\n","        else:\n","            log_message(\"[Error] Export skipped due to segmentation failure\")\n","\n","        return file, file_results\n","\n","    except Exception as e:\n","        file_results['messages'].append(f\"[Error] Unexpected failure: {e}\")\n","        return file, file_results\n","\n","def run_tests():\n","    import numpy as np\n","    print(\"=== Running Basic Functional Tests ===\")\n","    sr = 48000\n","    tone = 0.5 * np.sin(2 * np.pi * 440 * np.linspace(0,1,sr))\n","    silence = np.zeros(int(0.5 * sr))\n","    y = np.concatenate([silence, tone, silence]).astype(np.float32)\n","\n","    y_trim, (pre,post,total), (s0,e0) = process_silence(y, sr, cfg.use_cuda, cfg.device, print)\n","    print(f\"Trim results: pre={pre:.3f}s, post={post:.3f}s, total={total:.3f}s\")\n","\n","    stereo = np.vstack([y,y])\n","    lp,rp,dev = pan_percent(stereo[0], stereo[1])\n","    print(f\"Pan percent: L={lp:.1f}%, R={rp:.1f}%, dev={dev:.1f}%\")\n","\n","    global TARGET_LUFS, TARGET_PEAK_DB\n","    sl,sp = cfg.TARGET_LUFS, cfg.TARGET_PEAK_DB\n","    TARGET_LUFS = -14.0; TARGET_PEAK_DB = -3.0\n","    try:\n","        y_norm, report = normalize_loudness_true(y_trim, sr, print)\n","        print(f\"Norm report: {report}\")\n","    finally:\n","        TARGET_LUFS, TARGET_PEAK_DB = sl, sp\n","\n","    print(\"Basic functional tests completed.\")\n","\n","def main():\n","    tqdm.write(f\"[Info] Export path: {export}\")\n","    files = get_all_audio_files(input_path)\n","    tqdm.write(f\"[Info] Processing {len(files)} audio file(s)\")\n","\n","    params = {\n","        'sample_rate': sample_rate,\n","        'segmentation': cfg.segmentation,\n","        'time_unit': time_unit,\n","        'duration': duration,\n","        'normalize_audio': normalize_audio,\n","        'panning_correction': cfg.panning_correction,\n","        'duration_seconds': cfg.duration_seconds,\n","        'use_cuda': cfg.use_cuda,\n","        'device': cfg.device\n","    }\n","\n","    results = {}\n","    failed_files = []\n","\n","    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n","        future_to_file = {executor.submit(process_file, f, params): f for f in files}\n","        for future in tqdm(concurrent.futures.as_completed(future_to_file),\n","                           total=len(files), desc=\"Processing files\"):\n","            f = future_to_file[future]\n","            try:\n","                file, file_results = future.result()\n","                results[file] = file_results\n","            except Exception as e:\n","                tqdm.write(f\"[Error] Processing {f} failed: {e}\")\n","                failed_files.append(f)\n","\n","    any_segmentation_failed = False\n","\n","    for file, file_results in results.items():\n","        for msg in file_results['messages']:\n","            tqdm.write(msg)\n","\n","        plot_data = file_results['plot_data']\n","        export_info = file_results['export_paths']\n","\n","        if plot_data['segmentation_failed']:\n","            any_segmentation_failed = True\n","\n","        if not plot_data['segmentation_failed']:\n","            if plot_data['current_segmentation'] and plot_data['segments']:\n","                plot_trim_boundaries(\n","                    plot_data['y_proc'],\n","                    plot_data['sr'],\n","                    0,\n","                    plot_data['total_len'],\n","                    plot_data['segments']\n","                )\n","            else:\n","                plot_trim_boundaries(\n","                    plot_data['y'],\n","                    plot_data['sr'],\n","                    plot_data['samp_start'],\n","                    plot_data['samp_end']\n","                )\n","\n","            plot_zoomed_silence(\n","                plot_data['y'],\n","                plot_data['sr'],\n","                plot_data['samp_start'],\n","                plot_data['samp_end']\n","            )\n","\n","            title = \"Normalized Waveform\" if normalize_audio else \"Processed Waveform (without normalization)\"\n","            plot_normalized_waveform(plot_data['y_proc'], plot_data['sr'], title)\n","\n","        total_segments_count = (\n","            len(export_info) if cfg.segmentation and not plot_data['segmentation_failed']\n","            else (1 if export_info else 0)\n","        )\n","\n","        for export_item in export_info:\n","            tqdm.write(\"\")  # Blank line before Exported Audio Details\n","            if export_item['seg_index'] is not None:\n","                tqdm.write(\"Exported Segment Details:\")\n","            else:\n","                tqdm.write(\"Exported Audio Details:\")\n","\n","            tqdm.write(f\"  Path: {export_item['path']}\")\n","            tqdm.write(f\"  Format: {out_format.upper()}\")\n","\n","            try:\n","                info_exp = sf.info(export_item['path'])\n","                tqdm.write(f\"  Sample Rate: {info_exp.samplerate} Hz\")\n","                tqdm.write(f\"  Bit Depth: {info_exp.subtype}\")\n","                channels_exp = info_exp.channels\n","                ch_str = \"Mono\" if channels_exp == 1 else \"Stereo\"\n","                tqdm.write(f\"  Channels: {ch_str}\")\n","            except Exception as e:\n","                tqdm.write(f\"  [Error] Could not read exported file metadata: {e}\")\n","\n","            if out_format.lower() == \"mp3\":\n","                tqdm.write(f\"  MP3 Bitrate: {cfg.mp3_bitrate}\")\n","\n","            if cfg.segmentation and not plot_data['segmentation_failed']:\n","                tqdm.write(f\"  Total Segments: {total_segments_count}\")\n","\n","            if cfg.segmentation:\n","                tqdm.write(f\"  Time Unit: {time_unit}\")\n","\n","            tqdm.write(f\"  Duration: {format_duration(export_item['seg_duration'])}\")\n","\n","            try:\n","                y_exp, sr_exp = librosa.load(export_item['path'], sr=None, mono=False)\n","                if y_exp.ndim > 1 and y_exp.shape[0] == 2:\n","                    l_pct_out, r_pct_out, _ = pan_percent(y_exp[0], y_exp[1])\n","                    tqdm.write(f\"  Pan Balance Output: Left = {l_pct_out:.2f}%, Right = {r_pct_out:.2f}%\")\n","                else:\n","                    tqdm.write(\"  Pan Balance Output: Left = 50.00%, Right = 50.00%\")\n","            except Exception as e:\n","                tqdm.write(f\"  [Error] Could not compute pan balance: {e}\")\n","\n","            if not normalize_audio and export_item['original_metrics'] is not None:\n","                metrics = export_item['original_metrics']\n","                if metrics['lufs'] is not None:\n","                    tqdm.write(f\"  Original LUFS: {metrics['lufs']:.2f} LUFS\")\n","                tqdm.write(f\"  Original Peak: {metrics['peak']:.2f} dBFS\")\n","\n","            norm_report = export_item['norm_report']\n","            if norm_report is not None:\n","                if norm_report['original_lufs'] is not None:\n","                    tqdm.write(f\"  Original LUFS: {norm_report['original_lufs']:.2f} LUFS\")\n","                else:\n","                    tqdm.write(\"  Original LUFS: Not measured\")\n","                if norm_report.get('original_peak') is not None:\n","                    tqdm.write(f\"  Original PEAK: {norm_report['original_peak']:.2f} dBFS\")\n","                else:\n","                    tqdm.write(\"  Original PEAK: N/A\")\n","                if norm_report['normalized_lufs'] is not None:\n","                    tqdm.write(f\"  Normalized LUFS: {norm_report['normalized_lufs']:.2f} LUFS\")\n","                else:\n","                    tqdm.write(\"  Normalized LUFS: Not measured\")\n","                tqdm.write(f\"  Normalized Peak: {norm_report['normalized_peak']:.2f} dBFS\")\n","            elif normalize_audio:\n","                try:\n","                    y_exp2, _ = librosa.load(export_item['path'], sr=None, mono=False)\n","                    peak_amp = np.max(np.abs(y_exp2))\n","                    peak_db = 20 * np.log10(peak_amp) if peak_amp > 0 else -np.inf\n","                    tqdm.write(f\"  Measured Peak: {peak_db:.2f} dBFS\")\n","                except:\n","                    tqdm.write(\"  [Error] Cannot measure peak\")\n","\n","            total_sil = file_results.get('total_silence', 0.0)\n","            tqdm.write(f\"  Total silence removed: {format_duration(total_sil)}\")\n","\n","            device_str = \"GPU\" if cfg.use_cuda else \"CPU\"\n","            tqdm.write(f\"  Processing Device: {device_str}\")\n","\n","    if failed_files:\n","        tqdm.write(\"[Error] Some files failed:\")\n","        for fn in failed_files:\n","            tqdm.write(f\"  - {fn}\")\n","\n","    if not any_segmentation_failed:\n","        tqdm.write(f\"[Info] Processing complete! Processed {len(files)-len(failed_files)} file(s) successfully.\")\n","    return results\n","\n","# === RUN TESTS + MAIN, then DISPLAY AUDIO PLAYERS ===\n","run_tests()\n","results = main()\n","\n","print(\"\\n\\n--- Preprocessed Audio Previews ---\\n\")\n","for file, file_results in results.items():\n","    for export_item in file_results['export_paths']:\n","        print(\"▶\", export_item['path'])\n","        display(ipd.Audio(export_item['path']))\n"]}]}
